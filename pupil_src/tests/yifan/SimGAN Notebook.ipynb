{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eb961520-81e7-da2c-5fa7-852f98749f3e"
   },
   "source": [
    "# Introduction\n",
    "## Overview\n",
    "The notebook applies the [SimGAN](https://arxiv.org/pdf/1612.07828v1.pdf) network architecture to the problem of generating realistic images of eyes by using real images to augment the simulated data. We use Keras with a Tensorflow-backend to accomplish the adversarial training. \n",
    "### Note \n",
    "The model is quite small and the training is tuned down substantially since the limits of Kaggle Kernels and the lack of GPU makes the training very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "bb0b3648-a55b-e9dd-7396-97fbedf4a3c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "d395fcbf-7846-00df-e949-728c55e5528b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-version 1.13.1 keras-version 2.0.3\n"
     ]
    }
   ],
   "source": [
    "print('tf-version',tf.__version__, 'keras-version', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de81853c-aaff-d745-0f05-57c6d89a1038"
   },
   "source": [
    "## Loading Data\n",
    "Here we setup the paths and load the data from the hdf5 files since there would otherwise be too many individual jpg/png images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "c4d71194-04e2-49f0-ad43-d4297fccf7b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic images found: 50\n",
      "(50, 240, 320, 1)\n",
      "Real Images found: 40\n",
      "(40, 240, 320, 1)\n"
     ]
    }
   ],
   "source": [
    "path = os.path.dirname(os.path.abspath('.'))\n",
    "data_dir = os.path.join('..', 'input')\n",
    "cache_dir = '.'\n",
    "\n",
    "# load the data file and extract dimensions\n",
    "# with h5py.File(os.path.join(data_dir,'gaze.h5'),'r') as t_file:\n",
    "#     print(list(t_file.keys()))\n",
    "#     assert 'image' in t_file, \"Images are missing\"\n",
    "#     assert 'look_vec' in t_file, \"Look vector is missing\"\n",
    "#     assert 'path' in t_file, \"Paths are missing\"\n",
    "#     print('Synthetic images found:',len(t_file['image']))\n",
    "    \n",
    "#     for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n",
    "#         print('image',ikey,'shape:',ival.shape)\n",
    "#         img_height, img_width = ival.shape\n",
    "#         img_channels = 1\n",
    "#     syn_image_stack = np.stack([np.expand_dims(a,-1) for a in t_file['image'].values()],0)\n",
    "#     print(syn_image_stack.shape)\n",
    "\n",
    "\n",
    "######## my syn ##########\n",
    "with h5py.File('syn.h5','r') as t_file:\n",
    "    assert 'image' in t_file, \"Images are missing\"\n",
    "    print('Synthetic images found:',len(t_file['image']))\n",
    "    for _, (ikey, ival) in zip(range(1), t_file.items()):\n",
    "        img_height = ival.shape[1]\n",
    "        img_width = ival.shape[2]\n",
    "        img_channels = 1\n",
    "    tmp = np.stack([np.expand_dims(a,-1) for a in t_file.values()],0)\n",
    "    syn_image_stack = tmp[0]\n",
    "    print(syn_image_stack.shape)\n",
    "\n",
    "# with h5py.File(os.path.join(data_dir,'real_gaze.h5'),'r') as t_file:\n",
    "#     print(list(t_file.keys()))\n",
    "#     assert 'image' in t_file, \"Images are missing\"\n",
    "#     print('Real Images found:',len(t_file['image']))\n",
    "#     for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n",
    "#         print('image',ikey,'shape:',ival.shape)\n",
    "#          img_height, img_width = ival.shape\n",
    "#         img_channels = 1\n",
    "#     real_image_stack = np.stack([np.expand_dims(a,-1) for a in t_file['image'].values()],0)\n",
    "\n",
    "######## my real #########\n",
    "with h5py.File('real.h5','r') as t_file:\n",
    "    assert 'image' in t_file, \"Images are missing\"\n",
    "    print('Real Images found:',len(t_file['image']))\n",
    "    for _, (ikey, ival) in zip(range(1), t_file.items()):\n",
    "        img_height = ival.shape[1]\n",
    "        img_width = ival.shape[2]\n",
    "        img_channels = 1\n",
    "    tmp1 = np.stack([np.expand_dims(a,-1) for a in t_file.values()],0)\n",
    "    real_image_stack = tmp1[0]\n",
    "    print(real_image_stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "c4d71194-04e2-49f0-ad43-d4297fccf7b2"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# training params\n",
    "#\n",
    "\n",
    "nb_steps = 2000 # originally 10000, but this makes the kernel time out\n",
    "batch_size = 5\n",
    "k_d = 1  # number of discriminator updates per step\n",
    "k_g = 2  # number of generative network updates per step\n",
    "log_interval = 20\n",
    "save_interval = 50\n",
    "pre_steps = 15 # for pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4881321d-64f8-fe2a-d8bb-1c74500754a4"
   },
   "source": [
    "## Utility Functions\n",
    "These functions make it a bit easier to keep track of how the training is going and make it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "da7514ef-2b69-8945-79da-fb0583dd36ea"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Module to plot a batch of images along w/ their corresponding label(s)/annotations and save the plot to disc.\n",
    "\n",
    "Use cases:\n",
    "Plot images along w/ their corresponding ground-truth label & model predicted label,\n",
    "Plot images generated by a GAN along w/ any annotations used to generate these images,\n",
    "Plot synthetic, generated, refined, and real images and see how they compare as training progresses in a GAN,\n",
    "etc...\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from itertools import groupby\n",
    "# from skimage.util.montage import montage2d\n",
    "from skimage.util import montage\n",
    "def plot_batch(image_batch, figure_path, label_batch=None):\n",
    "    \n",
    "    all_groups = {label: montage(np.stack([img[:,:,0] for img, lab in img_lab_list],0)) \n",
    "                  for label, img_lab_list in groupby(zip(image_batch, label_batch), lambda x: x[1])}\n",
    "    fig, c_axs = plt.subplots(1,len(all_groups), figsize=(len(all_groups)*4, 8), dpi = 600)\n",
    "    for c_ax, (c_label, c_mtg) in zip(c_axs, all_groups.items()):\n",
    "        c_ax.imshow(c_mtg, cmap='bone')\n",
    "        c_ax.set_title(c_label)\n",
    "        c_ax.axis('off')\n",
    "    fig.savefig(os.path.join(figure_path))\n",
    "    plt.close()\n",
    "\"\"\"\n",
    "Module implementing the image history buffer described in `2.3. Updating Discriminator using a History of\n",
    "Refined Images` of https://arxiv.org/pdf/1612.07828v1.pdf.\n",
    "\n",
    "\"\"\"\n",
    "class ImageHistoryBuffer(object):\n",
    "    def __init__(self, shape, max_size, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the class's state.\n",
    "\n",
    "        :param shape: Shape of the data to be stored in the image history buffer\n",
    "                      (i.e. (0, img_height, img_width, img_channels)).\n",
    "        :param max_size: Maximum number of images that can be stored in the image history buffer.\n",
    "        :param batch_size: Batch size used to train GAN.\n",
    "        \"\"\"\n",
    "        self.image_history_buffer = np.zeros(shape=shape)\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add_to_image_history_buffer(self, images, nb_to_add=None):\n",
    "        \"\"\"\n",
    "        To be called during training of GAN. By default add batch_size // 2 images to the image history buffer each\n",
    "        time the generator generates a new batch of images.\n",
    "\n",
    "        :param images: Array of images (usually a batch) to be added to the image history buffer.\n",
    "        :param nb_to_add: The number of images from `images` to add to the image history buffer\n",
    "                          (batch_size / 2 by default).\n",
    "        \"\"\"\n",
    "        if not nb_to_add:\n",
    "            nb_to_add = self.batch_size // 2\n",
    "\n",
    "        if len(self.image_history_buffer) < self.max_size:\n",
    "            np.append(self.image_history_buffer, images[:nb_to_add], axis=0)\n",
    "        elif len(self.image_history_buffer) == self.max_size:\n",
    "            self.image_history_buffer[:nb_to_add] = images[:nb_to_add]\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        np.random.shuffle(self.image_history_buffer)\n",
    "\n",
    "    def get_from_image_history_buffer(self, nb_to_get=None):\n",
    "        \"\"\"\n",
    "        Get a random sample of images from the history buffer.\n",
    "\n",
    "        :param nb_to_get: Number of images to get from the image history buffer (batch_size / 2 by default).\n",
    "        :return: A random sample of `nb_to_get` images from the image history buffer, or an empty np array if the image\n",
    "                 history buffer is empty.\n",
    "        \"\"\"\n",
    "        if not nb_to_get:\n",
    "            nb_to_get = self.batch_size // 2\n",
    "\n",
    "        try:\n",
    "            return self.image_history_buffer[:nb_to_get]\n",
    "        except IndexError:\n",
    "            return np.zeros(shape=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e8e42268-05b1-0e33-9fe1-f02e0a6d234e"
   },
   "source": [
    "# Network Architectures\n",
    "Here we define the two primary networks\n",
    "\n",
    " - refiner network\n",
    " - discriminator network\n",
    "\n",
    "They work against each other to constantly improve the results of the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "701fd9e1-f970-8420-504b-7c057e0173a2"
   },
   "outputs": [],
   "source": [
    "def refiner_network(input_image_tensor):\n",
    "    \"\"\"\n",
    "    The refiner network, Rθ, is a residual network (ResNet). It modifies the synthetic image on a pixel level, rather\n",
    "    than holistically modifying the image content, preserving the global structure and annotations.\n",
    "\n",
    "    :param input_image_tensor: Input tensor that corresponds to a synthetic image.\n",
    "    :return: Output tensor that corresponds to a refined synthetic image.\n",
    "    \"\"\"\n",
    "    def resnet_block(input_features, nb_features=64, nb_kernel_rows=3, nb_kernel_cols=3):\n",
    "        \"\"\"\n",
    "        A ResNet block with two `nb_kernel_rows` x `nb_kernel_cols` convolutional layers,\n",
    "        each with `nb_features` feature maps.\n",
    "\n",
    "        See Figure 6 in https://arxiv.org/pdf/1612.07828v1.pdf.\n",
    "\n",
    "        :param input_features: Input tensor to ResNet block.\n",
    "        :return: Output tensor from ResNet block.\n",
    "        \"\"\"\n",
    "        y = layers.Convolution2D(nb_features, nb_kernel_rows, nb_kernel_cols, border_mode='same')(input_features)\n",
    "        y = layers.Activation('relu')(y)\n",
    "        y = layers.Convolution2D(nb_features, nb_kernel_rows, nb_kernel_cols, border_mode='same')(y)\n",
    "\n",
    "        y = layers.merge([input_features, y], mode='sum')\n",
    "        return layers.Activation('relu')(y)\n",
    "\n",
    "    # an input image of size w × h is convolved with 3 × 3 filters that output 64 feature maps\n",
    "    x = layers.Convolution2D(64, 3, 3, border_mode='same', activation='relu')(input_image_tensor)\n",
    "\n",
    "    # the output is passed through 4 ResNet blocks\n",
    "    for _ in range(4):\n",
    "        x = resnet_block(x)\n",
    "\n",
    "    # the output of the last ResNet block is passed to a 1 × 1 convolutional layer producing 1 feature map\n",
    "    # corresponding to the refined synthetic image\n",
    "    return layers.Convolution2D(img_channels, 1, 1, border_mode='same', activation='tanh')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "35420452-e116-f91a-a386-c2f8f290effe"
   },
   "outputs": [],
   "source": [
    "def discriminator_network(input_image_tensor):\n",
    "    \"\"\"\n",
    "    The discriminator network, Dφ, contains 5 convolution layers and 2 max-pooling layers.\n",
    "\n",
    "    :param input_image_tensor: Input tensor corresponding to an image, either real or refined.\n",
    "    :return: Output tensor that corresponds to the probability of whether an image is real or refined.\n",
    "    \"\"\"\n",
    "    x = layers.Convolution2D(96, 3, 3, border_mode='same', subsample=(2, 2), activation='relu')(input_image_tensor)\n",
    "    x = layers.Convolution2D(64, 3, 3, border_mode='same', subsample=(2, 2), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), border_mode='same', strides=(1, 1))(x)\n",
    "    x = layers.Convolution2D(32, 3, 3, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "    x = layers.Convolution2D(32, 1, 1, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "    x = layers.Convolution2D(2, 1, 1, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "\n",
    "    # here one feature map corresponds to `is_real` and the other to `is_refined`,\n",
    "    # and the custom loss function is then `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "    return layers.Reshape((-1, 2))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "365c0d27-8e84-1561-2434-3bde90c31534"
   },
   "source": [
    "## Combining Models\n",
    "Adversarial training of refiner network Rθ and discriminator network Dφ and combining them into single loss-functions and models that can be trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "fe1d7259-a48d-bb66-e471-635ac4958124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\keras\\legacy\\layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (1, 1), activation=\"tanh\", padding=\"same\")`\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")`\n",
      "  \n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 240, 320, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 240, 320, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 240, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 240, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "merge_1 (Merge)              (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 240, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 240, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "merge_2 (Merge)              (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 240, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 240, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "merge_3 (Merge)              (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 240, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 240, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "merge_4 (Merge)              (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 240, 320, 1)       65        \n",
      "=================================================================\n",
      "Total params: 296,129\n",
      "Trainable params: 296,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 240, 320, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 120, 160, 96)      960       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 60, 80, 64)        55360     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 60, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 60, 80, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 60, 80, 32)        1056      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 60, 80, 2)         66        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4800, 2)           0         \n",
      "=================================================================\n",
      "Total params: 75,906\n",
      "Trainable params: 75,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 240, 320, 1)       0         \n",
      "_________________________________________________________________\n",
      "refiner (Model)              (None, 240, 320, 1)       296129    \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 4800, 2)           75906     \n",
      "=================================================================\n",
      "Total params: 372,035\n",
      "Trainable params: 372,035\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", strides=(1, 1), padding=\"same\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", strides=(1, 1), padding=\"same\")`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (1, 1), activation=\"relu\", strides=(1, 1), padding=\"same\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"refiner\", inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"discriminator\", inputs=Tensor(\"in..., outputs=Tensor(\"re...)`\n",
      "C:\\Users\\localadmin\\Anaconda3\\envs\\simgan\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"combined\", inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# define model input and output tensors\n",
    "#\n",
    "\n",
    "synthetic_image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "refined_image_tensor = refiner_network(synthetic_image_tensor)\n",
    "\n",
    "refined_or_real_image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "discriminator_output = discriminator_network(refined_or_real_image_tensor)\n",
    "\n",
    "#\n",
    "# define models\n",
    "#\n",
    "\n",
    "refiner_model = models.Model(input=synthetic_image_tensor, output=refined_image_tensor, name='refiner')\n",
    "discriminator_model = models.Model(input=refined_or_real_image_tensor, output=discriminator_output,\n",
    "                                   name='discriminator')\n",
    "\n",
    "# combined must output the refined image along w/ the disc's classification of it for the refiner's self-reg loss\n",
    "refiner_model_output = refiner_model(synthetic_image_tensor)\n",
    "combined_output = discriminator_model(refiner_model_output)\n",
    "combined_model = models.Model(input=synthetic_image_tensor, output=[refiner_model_output, combined_output],\n",
    "                              name='combined')\n",
    "\n",
    "discriminator_model_output_shape = discriminator_model.output_shape\n",
    "\n",
    "print(refiner_model.summary())\n",
    "print(discriminator_model.summary())\n",
    "print(combined_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "33ede9dc-63dc-e7eb-cabf-39ac01fcd5f4"
   },
   "source": [
    "## Visualization\n",
    "Here we show the network architectures visually with shapes (doesn't yet work on kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "44bc1cfc-a7f4-1aa4-432f-bef7473ff3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running the patched version of keras/pydot!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "# Define model\n",
    "try:\n",
    "    model_to_dot(refiner_model, show_shapes=True).write_svg('refiner_model.svg')\n",
    "    SVG('refiner_model.svg')\n",
    "except ImportError:\n",
    "    print('Not running the patched version of keras/pydot!')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "c2821044-015b-5b2e-4cf7-b22195158767"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model_to_dot(discriminator_model, show_shapes=True).write_svg('discriminator_model.svg')\n",
    "    SVG('discriminator_model.svg')\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "e7446d98-534f-1094-ee09-245f3f7d28f7"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# define custom l1 loss function for the refiner\n",
    "#\n",
    "\n",
    "def self_regularization_loss(y_true, y_pred):\n",
    "    delta = 0.0001  # FIXME: need to figure out an appropriate value for this\n",
    "    return tf.multiply(delta, tf.reduce_sum(tf.abs(y_pred - y_true)))\n",
    "\n",
    "#\n",
    "# define custom local adversarial loss (softmax for each image section) for the discriminator\n",
    "# the adversarial loss function is the sum of the cross-entropy losses over the local patches\n",
    "#\n",
    "\n",
    "def local_adversarial_loss(y_true, y_pred):\n",
    "    # y_true and y_pred have shape (batch_size, # of local patches, 2), but really we just want to average over\n",
    "    # the local patches and batch size so we can reshape to (batch_size * # of local patches, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, 2))\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c70b7cb-dd4a-9a46-15f3-02d78fdc53f7"
   },
   "source": [
    "# Compile Models\n",
    "Here we combine the models and compile them with the loss functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "3faaf71b-0cfc-59e8-4b37-13ac79d61cfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-afb1781a18f1>:19: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = optimizers.SGD(lr=1e-3)\n",
    "\n",
    "refiner_model.compile(optimizer=sgd, loss=self_regularization_loss)\n",
    "discriminator_model.compile(optimizer=sgd, loss=local_adversarial_loss)\n",
    "discriminator_model.trainable = False\n",
    "combined_model.compile(optimizer=sgd, loss=[self_regularization_loss, local_adversarial_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "d50c46c7-94cf-ba72-ab47-f9dd0a8f36c3"
   },
   "outputs": [],
   "source": [
    "refiner_model_path = None\n",
    "discriminator_model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb536531-09b4-d9b7-a23e-f3a82517939f"
   },
   "source": [
    "# Data Generators\n",
    "Here we setup the pipeline to feed new images to both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "ba8320cb-5aef-f682-0e95-48fd2a319ea5"
   },
   "outputs": [],
   "source": [
    "datagen = image.ImageDataGenerator(\n",
    "    preprocessing_function=applications.xception.preprocess_input,\n",
    "    data_format='channels_last')\n",
    "\n",
    "flow_from_directory_params = {'target_size': (img_height, img_width),\n",
    "                              'color_mode': 'grayscale' if img_channels == 1 else 'rgb',\n",
    "                              'class_mode': None,\n",
    "                              'batch_size': batch_size}\n",
    "flow_params = {'batch_size': batch_size}\n",
    "\n",
    "synthetic_generator = datagen.flow(\n",
    "    x = syn_image_stack,\n",
    "    **flow_params\n",
    ")\n",
    "\n",
    "real_generator = datagen.flow(\n",
    "    x = real_image_stack,\n",
    "    **flow_params\n",
    ")\n",
    "\n",
    "def get_image_batch(generator):\n",
    "    \"\"\"keras generators may generate an incomplete batch for the last batch\"\"\"\n",
    "    img_batch = generator.next()\n",
    "    if len(img_batch) != batch_size:\n",
    "        img_batch = generator.next()\n",
    "\n",
    "    assert len(img_batch) == batch_size\n",
    "\n",
    "    return img_batch\n",
    "\n",
    "# the target labels for the cross-entropy loss layer are 0 for every yj (real) and 1 for every xi (refined)\n",
    "y_real = np.array([[[1.0, 0.0]] * discriminator_model_output_shape[1]] * batch_size)\n",
    "y_refined = np.array([[[0.0, 1.0]] * discriminator_model_output_shape[1]] * batch_size)\n",
    "assert y_real.shape == (batch_size, discriminator_model_output_shape[1], 2)\n",
    "batch_out = get_image_batch(synthetic_generator)\n",
    "assert batch_out.shape == (batch_size, img_height, img_width, img_channels), \"Image Dimensions do not match, {}!={}\".format(batch_out.shape, (batch_size, img_height, img_width, img_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3670cae4-d254-3648-4a86-ac7e1e585ff2"
   },
   "source": [
    "# Pretraining\n",
    "Here we pretraining the models before we start the full-blown training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "22ceec0a-0697-d1ce-05b5-0cb68851edbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-training the refiner network...\n",
      "Saving batch of refined images during pre-training at step: 0.\n",
      "Refiner model self regularization loss: [0.55880656].\n",
      "pre-training the discriminator network...\n",
      "Discriminator model loss: [0.10464593].\n"
     ]
    }
   ],
   "source": [
    "if not refiner_model_path:\n",
    "    # we first train the Rθ network with just self-regularization loss for 1,000 steps\n",
    "    print('pre-training the refiner network...')\n",
    "    gen_loss = np.zeros(shape=len(refiner_model.metrics_names))\n",
    "\n",
    "    for i in range(pre_steps):\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        gen_loss = np.add(refiner_model.train_on_batch(synthetic_image_batch, synthetic_image_batch), gen_loss)\n",
    "\n",
    "        # log every `log_interval` steps\n",
    "        if not i % log_interval:\n",
    "            figure_name = 'refined_image_batch_pre_train_step_{}.png'.format(i)\n",
    "            print('Saving batch of refined images during pre-training at step: {}.'.format(i))\n",
    "\n",
    "            synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "            plot_batch(\n",
    "                np.concatenate((synthetic_image_batch, refiner_model.predict_on_batch(synthetic_image_batch))),\n",
    "                os.path.join(cache_dir, figure_name),\n",
    "                label_batch=['Synthetic'] * batch_size + ['Refined'] * batch_size)\n",
    "\n",
    "            print('Refiner model self regularization loss: {}.'.format(gen_loss / log_interval))\n",
    "            gen_loss = np.zeros(shape=len(refiner_model.metrics_names))\n",
    "\n",
    "    refiner_model.save(os.path.join(cache_dir, 'refiner_model_pre_trained.h5'))\n",
    "else:\n",
    "    refiner_model.load_weights(refiner_model_path)\n",
    "\n",
    "if not discriminator_model_path:\n",
    "    # and Dφ for 200 steps (one mini-batch for refined images, another for real)\n",
    "    print('pre-training the discriminator network...')\n",
    "    disc_loss = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "    for _ in range(pre_steps):\n",
    "        real_image_batch = get_image_batch(real_generator)\n",
    "        disc_loss = np.add(discriminator_model.train_on_batch(real_image_batch, y_real), disc_loss)\n",
    "\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        refined_image_batch = refiner_model.predict_on_batch(synthetic_image_batch)\n",
    "        disc_loss = np.add(discriminator_model.train_on_batch(refined_image_batch, y_refined), disc_loss)\n",
    "\n",
    "    discriminator_model.save(os.path.join(cache_dir, 'discriminator_model_pre_trained.h5'))\n",
    "\n",
    "    # hard-coded for now\n",
    "    print('Discriminator model loss: {}.'.format(disc_loss / (100 * 2)))\n",
    "else:\n",
    "    discriminator_model.load_weights(discriminator_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Refiner_model_loss = []\n",
    "Discriminator_model_loss_real = []\n",
    "Discriminator_model_loss_refined = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f7acaeb-5a0e-d418-b855-b3e85b7854c0"
   },
   "source": [
    "# Full Training\n",
    "Here we run the full training of the model (normally for thousands of iterations, but here just a few). The images are saved in the output directory for examining the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "6d7fb313-ec78-659b-0510-b6a235400294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 of 2000.\n",
      "Step: 1 of 2000.\n",
      "Step: 2 of 2000.\n",
      "Step: 3 of 2000.\n",
      "Step: 4 of 2000.\n",
      "Step: 5 of 2000.\n",
      "Step: 6 of 2000.\n",
      "Step: 7 of 2000.\n",
      "Step: 8 of 2000.\n",
      "Step: 9 of 2000.\n",
      "Step: 10 of 2000.\n",
      "Step: 11 of 2000.\n",
      "Step: 12 of 2000.\n",
      "Step: 13 of 2000.\n",
      "Step: 14 of 2000.\n",
      "Step: 15 of 2000.\n",
      "Step: 16 of 2000.\n",
      "Step: 17 of 2000.\n",
      "Step: 18 of 2000.\n",
      "Step: 19 of 2000.\n",
      "Saving batch of refined images at adversarial step: 19.\n",
      "Refiner model loss: [3.41901793 2.7146315  0.70438643].\n",
      "Discriminator model loss real: [0.35457378].\n",
      "Discriminator model loss refined: [0.34108597].\n",
      "Step: 20 of 2000.\n",
      "Step: 21 of 2000.\n",
      "Step: 22 of 2000.\n",
      "Step: 23 of 2000.\n",
      "Step: 24 of 2000.\n",
      "Step: 25 of 2000.\n",
      "Step: 26 of 2000.\n",
      "Step: 27 of 2000.\n",
      "Step: 28 of 2000.\n",
      "Step: 29 of 2000.\n",
      "Step: 30 of 2000.\n",
      "Step: 31 of 2000.\n",
      "Step: 32 of 2000.\n",
      "Step: 33 of 2000.\n",
      "Step: 34 of 2000.\n",
      "Step: 35 of 2000.\n",
      "Step: 36 of 2000.\n",
      "Step: 37 of 2000.\n",
      "Step: 38 of 2000.\n",
      "Step: 39 of 2000.\n",
      "Saving batch of refined images at adversarial step: 39.\n",
      "Refiner model loss: [2.87658793 2.17296292 0.70362502].\n",
      "Discriminator model loss real: [0.35271755].\n",
      "Discriminator model loss refined: [0.34126682].\n",
      "Step: 40 of 2000.\n",
      "Step: 41 of 2000.\n",
      "Step: 42 of 2000.\n",
      "Step: 43 of 2000.\n",
      "Step: 44 of 2000.\n",
      "Step: 45 of 2000.\n",
      "Step: 46 of 2000.\n",
      "Step: 47 of 2000.\n",
      "Step: 48 of 2000.\n",
      "Step: 49 of 2000.\n",
      "Step: 50 of 2000.\n",
      "Step: 51 of 2000.\n",
      "Step: 52 of 2000.\n",
      "Step: 53 of 2000.\n",
      "Step: 54 of 2000.\n",
      "Step: 55 of 2000.\n",
      "Step: 56 of 2000.\n",
      "Step: 57 of 2000.\n",
      "Step: 58 of 2000.\n",
      "Step: 59 of 2000.\n",
      "Saving batch of refined images at adversarial step: 59.\n",
      "Refiner model loss: [2.65801373 1.95526252 0.70275122].\n",
      "Discriminator model loss real: [0.3510747].\n",
      "Discriminator model loss refined: [0.34155705].\n",
      "Step: 60 of 2000.\n",
      "Step: 61 of 2000.\n",
      "Step: 62 of 2000.\n",
      "Step: 63 of 2000.\n",
      "Step: 64 of 2000.\n",
      "Step: 65 of 2000.\n",
      "Step: 66 of 2000.\n",
      "Step: 67 of 2000.\n",
      "Step: 68 of 2000.\n",
      "Step: 69 of 2000.\n",
      "Step: 70 of 2000.\n",
      "Step: 71 of 2000.\n",
      "Step: 72 of 2000.\n",
      "Step: 73 of 2000.\n",
      "Step: 74 of 2000.\n",
      "Step: 75 of 2000.\n",
      "Step: 76 of 2000.\n",
      "Step: 77 of 2000.\n",
      "Step: 78 of 2000.\n",
      "Step: 79 of 2000.\n",
      "Saving batch of refined images at adversarial step: 79.\n",
      "Refiner model loss: [2.52717914 1.82426566 0.70291349].\n",
      "Discriminator model loss real: [0.35038385].\n",
      "Discriminator model loss refined: [0.34143287].\n",
      "Step: 80 of 2000.\n",
      "Step: 81 of 2000.\n",
      "Step: 82 of 2000.\n",
      "Step: 83 of 2000.\n",
      "Step: 84 of 2000.\n",
      "Step: 85 of 2000.\n",
      "Step: 86 of 2000.\n",
      "Step: 87 of 2000.\n",
      "Step: 88 of 2000.\n",
      "Step: 89 of 2000.\n",
      "Step: 90 of 2000.\n",
      "Step: 91 of 2000.\n",
      "Step: 92 of 2000.\n",
      "Step: 93 of 2000.\n",
      "Step: 94 of 2000.\n",
      "Step: 95 of 2000.\n",
      "Step: 96 of 2000.\n",
      "Step: 97 of 2000.\n",
      "Step: 98 of 2000.\n",
      "Step: 99 of 2000.\n",
      "Saving batch of refined images at adversarial step: 99.\n",
      "Refiner model loss: [2.43503284 1.73092076 0.70411209].\n",
      "Discriminator model loss real: [0.35058101].\n",
      "Discriminator model loss refined: [0.34080136].\n",
      "Step: 100 of 2000.\n",
      "Step: 101 of 2000.\n",
      "Step: 102 of 2000.\n",
      "Step: 103 of 2000.\n",
      "Step: 104 of 2000.\n",
      "Step: 105 of 2000.\n",
      "Step: 106 of 2000.\n",
      "Step: 107 of 2000.\n",
      "Step: 108 of 2000.\n",
      "Step: 109 of 2000.\n",
      "Step: 110 of 2000.\n",
      "Step: 111 of 2000.\n",
      "Step: 112 of 2000.\n",
      "Step: 113 of 2000.\n",
      "Step: 114 of 2000.\n",
      "Step: 115 of 2000.\n",
      "Step: 116 of 2000.\n",
      "Step: 117 of 2000.\n",
      "Step: 118 of 2000.\n",
      "Step: 119 of 2000.\n",
      "Saving batch of refined images at adversarial step: 119.\n",
      "Refiner model loss: [2.37766836 1.67209258 0.7055758 ].\n",
      "Discriminator model loss real: [0.35100322].\n",
      "Discriminator model loss refined: [0.34013846].\n",
      "Step: 120 of 2000.\n",
      "Step: 121 of 2000.\n",
      "Step: 122 of 2000.\n",
      "Step: 123 of 2000.\n",
      "Step: 124 of 2000.\n",
      "Step: 125 of 2000.\n",
      "Step: 126 of 2000.\n",
      "Step: 127 of 2000.\n",
      "Step: 128 of 2000.\n",
      "Step: 129 of 2000.\n",
      "Step: 130 of 2000.\n",
      "Step: 131 of 2000.\n",
      "Step: 132 of 2000.\n",
      "Step: 133 of 2000.\n",
      "Step: 134 of 2000.\n",
      "Step: 135 of 2000.\n",
      "Step: 136 of 2000.\n",
      "Step: 137 of 2000.\n",
      "Step: 138 of 2000.\n",
      "Step: 139 of 2000.\n",
      "Saving batch of refined images at adversarial step: 139.\n",
      "Refiner model loss: [2.31592965 1.6090223  0.70690735].\n",
      "Discriminator model loss real: [0.35132572].\n",
      "Discriminator model loss refined: [0.33940508].\n",
      "Step: 140 of 2000.\n",
      "Step: 141 of 2000.\n",
      "Step: 142 of 2000.\n",
      "Step: 143 of 2000.\n",
      "Step: 144 of 2000.\n",
      "Step: 145 of 2000.\n",
      "Step: 146 of 2000.\n",
      "Step: 147 of 2000.\n",
      "Step: 148 of 2000.\n",
      "Step: 149 of 2000.\n",
      "Step: 150 of 2000.\n",
      "Step: 151 of 2000.\n",
      "Step: 152 of 2000.\n",
      "Step: 153 of 2000.\n",
      "Step: 154 of 2000.\n",
      "Step: 155 of 2000.\n",
      "Step: 156 of 2000.\n",
      "Step: 157 of 2000.\n",
      "Step: 158 of 2000.\n",
      "Step: 159 of 2000.\n",
      "Saving batch of refined images at adversarial step: 159.\n",
      "Refiner model loss: [2.26889547 1.56061492 0.70828056].\n",
      "Discriminator model loss real: [0.35170652].\n",
      "Discriminator model loss refined: [0.33866848].\n",
      "Step: 160 of 2000.\n",
      "Step: 161 of 2000.\n",
      "Step: 162 of 2000.\n",
      "Step: 163 of 2000.\n",
      "Step: 164 of 2000.\n",
      "Step: 165 of 2000.\n",
      "Step: 166 of 2000.\n",
      "Step: 167 of 2000.\n",
      "Step: 168 of 2000.\n",
      "Step: 169 of 2000.\n",
      "Step: 170 of 2000.\n",
      "Step: 171 of 2000.\n",
      "Step: 172 of 2000.\n",
      "Step: 173 of 2000.\n",
      "Step: 174 of 2000.\n",
      "Step: 175 of 2000.\n",
      "Step: 176 of 2000.\n",
      "Step: 177 of 2000.\n",
      "Step: 178 of 2000.\n",
      "Step: 179 of 2000.\n",
      "Saving batch of refined images at adversarial step: 179.\n",
      "Refiner model loss: [2.22446511 1.51471154 0.70975356].\n",
      "Discriminator model loss real: [0.35209727].\n",
      "Discriminator model loss refined: [0.33797906].\n",
      "Step: 180 of 2000.\n",
      "Step: 181 of 2000.\n",
      "Step: 182 of 2000.\n",
      "Step: 183 of 2000.\n",
      "Step: 184 of 2000.\n",
      "Step: 185 of 2000.\n",
      "Step: 186 of 2000.\n",
      "Step: 187 of 2000.\n",
      "Step: 188 of 2000.\n",
      "Step: 189 of 2000.\n",
      "Step: 190 of 2000.\n",
      "Step: 191 of 2000.\n",
      "Step: 192 of 2000.\n",
      "Step: 193 of 2000.\n",
      "Step: 194 of 2000.\n",
      "Step: 195 of 2000.\n",
      "Step: 196 of 2000.\n",
      "Step: 197 of 2000.\n",
      "Step: 198 of 2000.\n",
      "Step: 199 of 2000.\n",
      "Saving batch of refined images at adversarial step: 199.\n",
      "Refiner model loss: [2.19018975 1.47890019 0.71128958].\n",
      "Discriminator model loss real: [0.35254773].\n",
      "Discriminator model loss refined: [0.33725394].\n",
      "Step: 200 of 2000.\n",
      "Step: 201 of 2000.\n",
      "Step: 202 of 2000.\n",
      "Step: 203 of 2000.\n",
      "Step: 204 of 2000.\n",
      "Step: 205 of 2000.\n",
      "Step: 206 of 2000.\n",
      "Step: 207 of 2000.\n",
      "Step: 208 of 2000.\n",
      "Step: 209 of 2000.\n",
      "Step: 210 of 2000.\n",
      "Step: 211 of 2000.\n",
      "Step: 212 of 2000.\n",
      "Step: 213 of 2000.\n",
      "Step: 214 of 2000.\n",
      "Step: 215 of 2000.\n",
      "Step: 216 of 2000.\n",
      "Step: 217 of 2000.\n",
      "Step: 218 of 2000.\n",
      "Step: 219 of 2000.\n",
      "Saving batch of refined images at adversarial step: 219.\n",
      "Refiner model loss: [2.16254904 1.44967069 0.71287834].\n",
      "Discriminator model loss real: [0.35305424].\n",
      "Discriminator model loss refined: [0.33646297].\n",
      "Step: 220 of 2000.\n",
      "Step: 221 of 2000.\n",
      "Step: 222 of 2000.\n",
      "Step: 223 of 2000.\n",
      "Step: 224 of 2000.\n",
      "Step: 225 of 2000.\n",
      "Step: 226 of 2000.\n",
      "Step: 227 of 2000.\n",
      "Step: 228 of 2000.\n",
      "Step: 229 of 2000.\n",
      "Step: 230 of 2000.\n",
      "Step: 231 of 2000.\n",
      "Step: 232 of 2000.\n",
      "Step: 233 of 2000.\n",
      "Step: 234 of 2000.\n",
      "Step: 235 of 2000.\n",
      "Step: 236 of 2000.\n",
      "Step: 237 of 2000.\n",
      "Step: 238 of 2000.\n",
      "Step: 239 of 2000.\n",
      "Saving batch of refined images at adversarial step: 239.\n",
      "Refiner model loss: [2.13324727 1.41859841 0.71464887].\n",
      "Discriminator model loss real: [0.35347287].\n",
      "Discriminator model loss refined: [0.33578252].\n",
      "Step: 240 of 2000.\n",
      "Step: 241 of 2000.\n",
      "Step: 242 of 2000.\n",
      "Step: 243 of 2000.\n",
      "Step: 244 of 2000.\n",
      "Step: 245 of 2000.\n",
      "Step: 246 of 2000.\n",
      "Step: 247 of 2000.\n",
      "Step: 248 of 2000.\n",
      "Step: 249 of 2000.\n",
      "Step: 250 of 2000.\n",
      "Step: 251 of 2000.\n",
      "Step: 252 of 2000.\n",
      "Step: 253 of 2000.\n",
      "Step: 254 of 2000.\n",
      "Step: 255 of 2000.\n",
      "Step: 256 of 2000.\n",
      "Step: 257 of 2000.\n",
      "Step: 258 of 2000.\n",
      "Step: 259 of 2000.\n",
      "Saving batch of refined images at adversarial step: 259.\n",
      "Refiner model loss: [2.11406736 1.39758598 0.71648139].\n",
      "Discriminator model loss real: [0.35413746].\n",
      "Discriminator model loss refined: [0.33490538].\n",
      "Step: 260 of 2000.\n",
      "Step: 261 of 2000.\n",
      "Step: 262 of 2000.\n",
      "Step: 263 of 2000.\n",
      "Step: 264 of 2000.\n",
      "Step: 265 of 2000.\n",
      "Step: 266 of 2000.\n",
      "Step: 267 of 2000.\n",
      "Step: 268 of 2000.\n",
      "Step: 269 of 2000.\n",
      "Step: 270 of 2000.\n",
      "Step: 271 of 2000.\n",
      "Step: 272 of 2000.\n",
      "Step: 273 of 2000.\n",
      "Step: 274 of 2000.\n",
      "Step: 275 of 2000.\n",
      "Step: 276 of 2000.\n",
      "Step: 277 of 2000.\n",
      "Step: 278 of 2000.\n",
      "Step: 279 of 2000.\n",
      "Saving batch of refined images at adversarial step: 279.\n",
      "Refiner model loss: [2.08545489 1.3672345  0.71822039].\n",
      "Discriminator model loss real: [0.35453966].\n",
      "Discriminator model loss refined: [0.3340169].\n",
      "Step: 280 of 2000.\n",
      "Step: 281 of 2000.\n",
      "Step: 282 of 2000.\n",
      "Step: 283 of 2000.\n",
      "Step: 284 of 2000.\n",
      "Step: 285 of 2000.\n",
      "Step: 286 of 2000.\n",
      "Step: 287 of 2000.\n",
      "Step: 288 of 2000.\n",
      "Step: 289 of 2000.\n",
      "Step: 290 of 2000.\n",
      "Step: 291 of 2000.\n",
      "Step: 292 of 2000.\n",
      "Step: 293 of 2000.\n",
      "Step: 294 of 2000.\n",
      "Step: 295 of 2000.\n",
      "Step: 296 of 2000.\n",
      "Step: 297 of 2000.\n",
      "Step: 298 of 2000.\n",
      "Step: 299 of 2000.\n",
      "Saving batch of refined images at adversarial step: 299.\n",
      "Refiner model loss: [2.06629115 1.34632918 0.71996199].\n",
      "Discriminator model loss real: [0.35506404].\n",
      "Discriminator model loss refined: [0.33317394].\n",
      "Step: 300 of 2000.\n",
      "Step: 301 of 2000.\n",
      "Step: 302 of 2000.\n",
      "Step: 303 of 2000.\n",
      "Step: 304 of 2000.\n",
      "Step: 305 of 2000.\n",
      "Step: 306 of 2000.\n",
      "Step: 307 of 2000.\n",
      "Step: 308 of 2000.\n",
      "Step: 309 of 2000.\n",
      "Step: 310 of 2000.\n",
      "Step: 311 of 2000.\n",
      "Step: 312 of 2000.\n",
      "Step: 313 of 2000.\n",
      "Step: 314 of 2000.\n",
      "Step: 315 of 2000.\n",
      "Step: 316 of 2000.\n",
      "Step: 317 of 2000.\n",
      "Step: 318 of 2000.\n",
      "Step: 319 of 2000.\n",
      "Saving batch of refined images at adversarial step: 319.\n",
      "Refiner model loss: [2.04564769 1.32393261 0.72171507].\n",
      "Discriminator model loss real: [0.35564848].\n",
      "Discriminator model loss refined: [0.33240827].\n",
      "Step: 320 of 2000.\n",
      "Step: 321 of 2000.\n",
      "Step: 322 of 2000.\n",
      "Step: 323 of 2000.\n",
      "Step: 324 of 2000.\n",
      "Step: 325 of 2000.\n",
      "Step: 326 of 2000.\n",
      "Step: 327 of 2000.\n",
      "Step: 328 of 2000.\n",
      "Step: 329 of 2000.\n",
      "Step: 330 of 2000.\n",
      "Step: 331 of 2000.\n",
      "Step: 332 of 2000.\n",
      "Step: 333 of 2000.\n",
      "Step: 334 of 2000.\n",
      "Step: 335 of 2000.\n",
      "Step: 336 of 2000.\n",
      "Step: 337 of 2000.\n",
      "Step: 338 of 2000.\n",
      "Step: 339 of 2000.\n",
      "Saving batch of refined images at adversarial step: 339.\n",
      "Refiner model loss: [2.03129782 1.307942   0.72335581].\n",
      "Discriminator model loss real: [0.35610849].\n",
      "Discriminator model loss refined: [0.33145313].\n",
      "Step: 340 of 2000.\n",
      "Step: 341 of 2000.\n",
      "Step: 342 of 2000.\n",
      "Step: 343 of 2000.\n",
      "Step: 344 of 2000.\n",
      "Step: 345 of 2000.\n",
      "Step: 346 of 2000.\n",
      "Step: 347 of 2000.\n",
      "Step: 348 of 2000.\n",
      "Step: 349 of 2000.\n",
      "Step: 350 of 2000.\n",
      "Step: 351 of 2000.\n",
      "Step: 352 of 2000.\n",
      "Step: 353 of 2000.\n",
      "Step: 354 of 2000.\n",
      "Step: 355 of 2000.\n",
      "Step: 356 of 2000.\n",
      "Step: 357 of 2000.\n",
      "Step: 358 of 2000.\n",
      "Step: 359 of 2000.\n",
      "Saving batch of refined images at adversarial step: 359.\n",
      "Refiner model loss: [2.01026899 1.28514955 0.72511944].\n",
      "Discriminator model loss real: [0.35668113].\n",
      "Discriminator model loss refined: [0.33071379].\n",
      "Step: 360 of 2000.\n",
      "Step: 361 of 2000.\n",
      "Step: 362 of 2000.\n",
      "Step: 363 of 2000.\n",
      "Step: 364 of 2000.\n",
      "Step: 365 of 2000.\n",
      "Step: 366 of 2000.\n",
      "Step: 367 of 2000.\n",
      "Step: 368 of 2000.\n",
      "Step: 369 of 2000.\n",
      "Step: 370 of 2000.\n",
      "Step: 371 of 2000.\n",
      "Step: 372 of 2000.\n",
      "Step: 373 of 2000.\n",
      "Step: 374 of 2000.\n",
      "Step: 375 of 2000.\n",
      "Step: 376 of 2000.\n",
      "Step: 377 of 2000.\n",
      "Step: 378 of 2000.\n",
      "Step: 379 of 2000.\n",
      "Saving batch of refined images at adversarial step: 379.\n",
      "Refiner model loss: [2.00112334 1.27430645 0.72681689].\n",
      "Discriminator model loss real: [0.35720647].\n",
      "Discriminator model loss refined: [0.33001749].\n",
      "Step: 380 of 2000.\n",
      "Step: 381 of 2000.\n",
      "Step: 382 of 2000.\n",
      "Step: 383 of 2000.\n",
      "Step: 384 of 2000.\n",
      "Step: 385 of 2000.\n",
      "Step: 386 of 2000.\n",
      "Step: 387 of 2000.\n",
      "Step: 388 of 2000.\n",
      "Step: 389 of 2000.\n",
      "Step: 390 of 2000.\n",
      "Step: 391 of 2000.\n",
      "Step: 392 of 2000.\n",
      "Step: 393 of 2000.\n",
      "Step: 394 of 2000.\n",
      "Step: 395 of 2000.\n",
      "Step: 396 of 2000.\n",
      "Step: 397 of 2000.\n",
      "Step: 398 of 2000.\n",
      "Step: 399 of 2000.\n",
      "Saving batch of refined images at adversarial step: 399.\n",
      "Refiner model loss: [1.98213027 1.25365412 0.72847615].\n",
      "Discriminator model loss real: [0.35765224].\n",
      "Discriminator model loss refined: [0.32913584].\n",
      "Step: 400 of 2000.\n",
      "Step: 401 of 2000.\n",
      "Step: 402 of 2000.\n",
      "Step: 403 of 2000.\n",
      "Step: 404 of 2000.\n",
      "Step: 405 of 2000.\n",
      "Step: 406 of 2000.\n",
      "Step: 407 of 2000.\n",
      "Step: 408 of 2000.\n",
      "Step: 409 of 2000.\n",
      "Step: 410 of 2000.\n",
      "Step: 411 of 2000.\n",
      "Step: 412 of 2000.\n",
      "Step: 413 of 2000.\n",
      "Step: 414 of 2000.\n",
      "Step: 415 of 2000.\n",
      "Step: 416 of 2000.\n",
      "Step: 417 of 2000.\n",
      "Step: 418 of 2000.\n",
      "Step: 419 of 2000.\n",
      "Saving batch of refined images at adversarial step: 419.\n",
      "Refiner model loss: [1.97301547 1.24284154 0.73017392].\n",
      "Discriminator model loss real: [0.35818776].\n",
      "Discriminator model loss refined: [0.32831862].\n",
      "Step: 420 of 2000.\n",
      "Step: 421 of 2000.\n",
      "Step: 422 of 2000.\n",
      "Step: 423 of 2000.\n",
      "Step: 424 of 2000.\n",
      "Step: 425 of 2000.\n",
      "Step: 426 of 2000.\n",
      "Step: 427 of 2000.\n",
      "Step: 428 of 2000.\n",
      "Step: 429 of 2000.\n",
      "Step: 430 of 2000.\n",
      "Step: 431 of 2000.\n",
      "Step: 432 of 2000.\n",
      "Step: 433 of 2000.\n",
      "Step: 434 of 2000.\n",
      "Step: 435 of 2000.\n",
      "Step: 436 of 2000.\n",
      "Step: 437 of 2000.\n",
      "Step: 438 of 2000.\n",
      "Step: 439 of 2000.\n",
      "Saving batch of refined images at adversarial step: 439.\n",
      "Refiner model loss: [1.95937523 1.22751147 0.73186377].\n",
      "Discriminator model loss real: [0.3587262].\n",
      "Discriminator model loss refined: [0.32757668].\n",
      "Step: 440 of 2000.\n",
      "Step: 441 of 2000.\n",
      "Step: 442 of 2000.\n",
      "Step: 443 of 2000.\n",
      "Step: 444 of 2000.\n",
      "Step: 445 of 2000.\n",
      "Step: 446 of 2000.\n",
      "Step: 447 of 2000.\n",
      "Step: 448 of 2000.\n",
      "Step: 449 of 2000.\n",
      "Step: 450 of 2000.\n",
      "Step: 451 of 2000.\n",
      "Step: 452 of 2000.\n",
      "Step: 453 of 2000.\n",
      "Step: 454 of 2000.\n",
      "Step: 455 of 2000.\n",
      "Step: 456 of 2000.\n",
      "Step: 457 of 2000.\n",
      "Step: 458 of 2000.\n",
      "Step: 459 of 2000.\n",
      "Saving batch of refined images at adversarial step: 459.\n",
      "Refiner model loss: [1.95051885 1.21686351 0.73365534].\n",
      "Discriminator model loss real: [0.35917608].\n",
      "Discriminator model loss refined: [0.32689325].\n",
      "Step: 460 of 2000.\n",
      "Step: 461 of 2000.\n",
      "Step: 462 of 2000.\n",
      "Step: 463 of 2000.\n",
      "Step: 464 of 2000.\n",
      "Step: 465 of 2000.\n",
      "Step: 466 of 2000.\n",
      "Step: 467 of 2000.\n",
      "Step: 468 of 2000.\n",
      "Step: 469 of 2000.\n",
      "Step: 470 of 2000.\n",
      "Step: 471 of 2000.\n",
      "Step: 472 of 2000.\n",
      "Step: 473 of 2000.\n",
      "Step: 474 of 2000.\n",
      "Step: 475 of 2000.\n",
      "Step: 476 of 2000.\n",
      "Step: 477 of 2000.\n",
      "Step: 478 of 2000.\n",
      "Step: 479 of 2000.\n",
      "Saving batch of refined images at adversarial step: 479.\n",
      "Refiner model loss: [1.9353483  1.20014201 0.73520629].\n",
      "Discriminator model loss real: [0.35972136].\n",
      "Discriminator model loss refined: [0.32604413].\n",
      "Step: 480 of 2000.\n",
      "Step: 481 of 2000.\n",
      "Step: 482 of 2000.\n",
      "Step: 483 of 2000.\n",
      "Step: 484 of 2000.\n",
      "Step: 485 of 2000.\n",
      "Step: 486 of 2000.\n",
      "Step: 487 of 2000.\n",
      "Step: 488 of 2000.\n",
      "Step: 489 of 2000.\n",
      "Step: 490 of 2000.\n",
      "Step: 491 of 2000.\n",
      "Step: 492 of 2000.\n",
      "Step: 493 of 2000.\n",
      "Step: 494 of 2000.\n",
      "Step: 495 of 2000.\n",
      "Step: 496 of 2000.\n",
      "Step: 497 of 2000.\n",
      "Step: 498 of 2000.\n",
      "Step: 499 of 2000.\n",
      "Saving batch of refined images at adversarial step: 499.\n",
      "Refiner model loss: [1.92884015 1.19186869 0.73697147].\n",
      "Discriminator model loss real: [0.36009503].\n",
      "Discriminator model loss refined: [0.3254403].\n",
      "Step: 500 of 2000.\n",
      "Step: 501 of 2000.\n",
      "Step: 502 of 2000.\n",
      "Step: 503 of 2000.\n",
      "Step: 504 of 2000.\n",
      "Step: 505 of 2000.\n",
      "Step: 506 of 2000.\n",
      "Step: 507 of 2000.\n",
      "Step: 508 of 2000.\n",
      "Step: 509 of 2000.\n",
      "Step: 510 of 2000.\n",
      "Step: 511 of 2000.\n",
      "Step: 512 of 2000.\n",
      "Step: 513 of 2000.\n",
      "Step: 514 of 2000.\n",
      "Step: 515 of 2000.\n",
      "Step: 516 of 2000.\n",
      "Step: 517 of 2000.\n",
      "Step: 518 of 2000.\n",
      "Step: 519 of 2000.\n",
      "Saving batch of refined images at adversarial step: 519.\n",
      "Refiner model loss: [1.91996784 1.18142092 0.73854691].\n",
      "Discriminator model loss real: [0.36073575].\n",
      "Discriminator model loss refined: [0.32457221].\n",
      "Step: 520 of 2000.\n",
      "Step: 521 of 2000.\n",
      "Step: 522 of 2000.\n",
      "Step: 523 of 2000.\n",
      "Step: 524 of 2000.\n",
      "Step: 525 of 2000.\n",
      "Step: 526 of 2000.\n",
      "Step: 527 of 2000.\n",
      "Step: 528 of 2000.\n",
      "Step: 529 of 2000.\n",
      "Step: 530 of 2000.\n",
      "Step: 531 of 2000.\n",
      "Step: 532 of 2000.\n",
      "Step: 533 of 2000.\n",
      "Step: 534 of 2000.\n",
      "Step: 535 of 2000.\n",
      "Step: 536 of 2000.\n",
      "Step: 537 of 2000.\n",
      "Step: 538 of 2000.\n",
      "Step: 539 of 2000.\n",
      "Saving batch of refined images at adversarial step: 539.\n",
      "Refiner model loss: [1.91149186 1.17146098 0.74003088].\n",
      "Discriminator model loss real: [0.3611272].\n",
      "Discriminator model loss refined: [0.32371871].\n",
      "Step: 540 of 2000.\n",
      "Step: 541 of 2000.\n",
      "Step: 542 of 2000.\n",
      "Step: 543 of 2000.\n",
      "Step: 544 of 2000.\n",
      "Step: 545 of 2000.\n",
      "Step: 546 of 2000.\n",
      "Step: 547 of 2000.\n",
      "Step: 548 of 2000.\n",
      "Step: 549 of 2000.\n",
      "Step: 550 of 2000.\n",
      "Step: 551 of 2000.\n",
      "Step: 552 of 2000.\n",
      "Step: 553 of 2000.\n",
      "Step: 554 of 2000.\n",
      "Step: 555 of 2000.\n",
      "Step: 556 of 2000.\n",
      "Step: 557 of 2000.\n",
      "Step: 558 of 2000.\n",
      "Step: 559 of 2000.\n",
      "Saving batch of refined images at adversarial step: 559.\n",
      "Refiner model loss: [1.90242957 1.16061765 0.74181192].\n",
      "Discriminator model loss real: [0.36181196].\n",
      "Discriminator model loss refined: [0.32321623].\n",
      "Step: 560 of 2000.\n",
      "Step: 561 of 2000.\n",
      "Step: 562 of 2000.\n",
      "Step: 563 of 2000.\n",
      "Step: 564 of 2000.\n",
      "Step: 565 of 2000.\n",
      "Step: 566 of 2000.\n",
      "Step: 567 of 2000.\n",
      "Step: 568 of 2000.\n",
      "Step: 569 of 2000.\n",
      "Step: 570 of 2000.\n",
      "Step: 571 of 2000.\n",
      "Step: 572 of 2000.\n",
      "Step: 573 of 2000.\n",
      "Step: 574 of 2000.\n",
      "Step: 575 of 2000.\n",
      "Step: 576 of 2000.\n",
      "Step: 577 of 2000.\n",
      "Step: 578 of 2000.\n",
      "Step: 579 of 2000.\n",
      "Saving batch of refined images at adversarial step: 579.\n",
      "Refiner model loss: [1.89972408 1.15625543 0.74346865].\n",
      "Discriminator model loss real: [0.36201613].\n",
      "Discriminator model loss refined: [0.32247152].\n",
      "Step: 580 of 2000.\n",
      "Step: 581 of 2000.\n",
      "Step: 582 of 2000.\n",
      "Step: 583 of 2000.\n",
      "Step: 584 of 2000.\n",
      "Step: 585 of 2000.\n",
      "Step: 586 of 2000.\n",
      "Step: 587 of 2000.\n",
      "Step: 588 of 2000.\n",
      "Step: 589 of 2000.\n",
      "Step: 590 of 2000.\n",
      "Step: 591 of 2000.\n",
      "Step: 592 of 2000.\n",
      "Step: 593 of 2000.\n",
      "Step: 594 of 2000.\n",
      "Step: 595 of 2000.\n",
      "Step: 596 of 2000.\n",
      "Step: 597 of 2000.\n",
      "Step: 598 of 2000.\n",
      "Step: 599 of 2000.\n",
      "Saving batch of refined images at adversarial step: 599.\n",
      "Refiner model loss: [1.89399944 1.14897093 0.74502851].\n",
      "Discriminator model loss real: [0.36267353].\n",
      "Discriminator model loss refined: [0.32178572].\n",
      "Step: 600 of 2000.\n",
      "Step: 601 of 2000.\n",
      "Step: 602 of 2000.\n",
      "Step: 603 of 2000.\n",
      "Step: 604 of 2000.\n",
      "Step: 605 of 2000.\n",
      "Step: 606 of 2000.\n",
      "Step: 607 of 2000.\n",
      "Step: 608 of 2000.\n",
      "Step: 609 of 2000.\n",
      "Step: 610 of 2000.\n",
      "Step: 611 of 2000.\n",
      "Step: 612 of 2000.\n",
      "Step: 613 of 2000.\n",
      "Step: 614 of 2000.\n",
      "Step: 615 of 2000.\n",
      "Step: 616 of 2000.\n",
      "Step: 617 of 2000.\n",
      "Step: 618 of 2000.\n",
      "Step: 619 of 2000.\n",
      "Saving batch of refined images at adversarial step: 619.\n",
      "Refiner model loss: [1.88124673 1.13469281 0.74655392].\n",
      "Discriminator model loss real: [0.36302229].\n",
      "Discriminator model loss refined: [0.32108524].\n",
      "Step: 620 of 2000.\n",
      "Step: 621 of 2000.\n",
      "Step: 622 of 2000.\n",
      "Step: 623 of 2000.\n",
      "Step: 624 of 2000.\n",
      "Step: 625 of 2000.\n",
      "Step: 626 of 2000.\n",
      "Step: 627 of 2000.\n",
      "Step: 628 of 2000.\n",
      "Step: 629 of 2000.\n",
      "Step: 630 of 2000.\n",
      "Step: 631 of 2000.\n",
      "Step: 632 of 2000.\n",
      "Step: 633 of 2000.\n",
      "Step: 634 of 2000.\n",
      "Step: 635 of 2000.\n",
      "Step: 636 of 2000.\n",
      "Step: 637 of 2000.\n",
      "Step: 638 of 2000.\n",
      "Step: 639 of 2000.\n",
      "Saving batch of refined images at adversarial step: 639.\n",
      "Refiner model loss: [1.87471101 1.12655634 0.74815467].\n",
      "Discriminator model loss real: [0.36352887].\n",
      "Discriminator model loss refined: [0.32039719].\n",
      "Step: 640 of 2000.\n",
      "Step: 641 of 2000.\n",
      "Step: 642 of 2000.\n",
      "Step: 643 of 2000.\n",
      "Step: 644 of 2000.\n",
      "Step: 645 of 2000.\n",
      "Step: 646 of 2000.\n",
      "Step: 647 of 2000.\n",
      "Step: 648 of 2000.\n",
      "Step: 649 of 2000.\n",
      "Step: 650 of 2000.\n",
      "Step: 651 of 2000.\n",
      "Step: 652 of 2000.\n",
      "Step: 653 of 2000.\n",
      "Step: 654 of 2000.\n",
      "Step: 655 of 2000.\n",
      "Step: 656 of 2000.\n",
      "Step: 657 of 2000.\n",
      "Step: 658 of 2000.\n",
      "Step: 659 of 2000.\n",
      "Saving batch of refined images at adversarial step: 659.\n",
      "Refiner model loss: [1.87080495 1.12115846 0.74964649].\n",
      "Discriminator model loss real: [0.36390432].\n",
      "Discriminator model loss refined: [0.31971434].\n",
      "Step: 660 of 2000.\n",
      "Step: 661 of 2000.\n",
      "Step: 662 of 2000.\n",
      "Step: 663 of 2000.\n",
      "Step: 664 of 2000.\n",
      "Step: 665 of 2000.\n",
      "Step: 666 of 2000.\n",
      "Step: 667 of 2000.\n",
      "Step: 668 of 2000.\n",
      "Step: 669 of 2000.\n",
      "Step: 670 of 2000.\n",
      "Step: 671 of 2000.\n",
      "Step: 672 of 2000.\n",
      "Step: 673 of 2000.\n",
      "Step: 674 of 2000.\n",
      "Step: 675 of 2000.\n",
      "Step: 676 of 2000.\n",
      "Step: 677 of 2000.\n",
      "Step: 678 of 2000.\n",
      "Step: 679 of 2000.\n",
      "Saving batch of refined images at adversarial step: 679.\n",
      "Refiner model loss: [1.86110099 1.10986574 0.75123525].\n",
      "Discriminator model loss real: [0.36458426].\n",
      "Discriminator model loss refined: [0.3189358].\n",
      "Step: 680 of 2000.\n",
      "Step: 681 of 2000.\n",
      "Step: 682 of 2000.\n",
      "Step: 683 of 2000.\n",
      "Step: 684 of 2000.\n",
      "Step: 685 of 2000.\n",
      "Step: 686 of 2000.\n",
      "Step: 687 of 2000.\n",
      "Step: 688 of 2000.\n",
      "Step: 689 of 2000.\n",
      "Step: 690 of 2000.\n",
      "Step: 691 of 2000.\n",
      "Step: 692 of 2000.\n",
      "Step: 693 of 2000.\n",
      "Step: 694 of 2000.\n",
      "Step: 695 of 2000.\n",
      "Step: 696 of 2000.\n",
      "Step: 697 of 2000.\n",
      "Step: 698 of 2000.\n",
      "Step: 699 of 2000.\n",
      "Saving batch of refined images at adversarial step: 699.\n",
      "Refiner model loss: [1.85417001 1.10147903 0.75269097].\n",
      "Discriminator model loss real: [0.3648387].\n",
      "Discriminator model loss refined: [0.31831977].\n",
      "Step: 700 of 2000.\n",
      "Step: 701 of 2000.\n",
      "Step: 702 of 2000.\n",
      "Step: 703 of 2000.\n",
      "Step: 704 of 2000.\n",
      "Step: 705 of 2000.\n",
      "Step: 706 of 2000.\n",
      "Step: 707 of 2000.\n",
      "Step: 708 of 2000.\n",
      "Step: 709 of 2000.\n",
      "Step: 710 of 2000.\n",
      "Step: 711 of 2000.\n",
      "Step: 712 of 2000.\n",
      "Step: 713 of 2000.\n",
      "Step: 714 of 2000.\n",
      "Step: 715 of 2000.\n",
      "Step: 716 of 2000.\n",
      "Step: 717 of 2000.\n",
      "Step: 718 of 2000.\n",
      "Step: 719 of 2000.\n",
      "Saving batch of refined images at adversarial step: 719.\n",
      "Refiner model loss: [1.85383792 1.0994882  0.75434972].\n",
      "Discriminator model loss real: [0.36530109].\n",
      "Discriminator model loss refined: [0.317868].\n",
      "Step: 720 of 2000.\n",
      "Step: 721 of 2000.\n",
      "Step: 722 of 2000.\n",
      "Step: 723 of 2000.\n",
      "Step: 724 of 2000.\n",
      "Step: 725 of 2000.\n",
      "Step: 726 of 2000.\n",
      "Step: 727 of 2000.\n",
      "Step: 728 of 2000.\n",
      "Step: 729 of 2000.\n",
      "Step: 730 of 2000.\n",
      "Step: 731 of 2000.\n",
      "Step: 732 of 2000.\n",
      "Step: 733 of 2000.\n",
      "Step: 734 of 2000.\n",
      "Step: 735 of 2000.\n",
      "Step: 736 of 2000.\n",
      "Step: 737 of 2000.\n",
      "Step: 738 of 2000.\n",
      "Step: 739 of 2000.\n",
      "Saving batch of refined images at adversarial step: 739.\n",
      "Refiner model loss: [1.8471181  1.09131049 0.75580761].\n",
      "Discriminator model loss real: [0.36556768].\n",
      "Discriminator model loss refined: [0.31699441].\n",
      "Step: 740 of 2000.\n",
      "Step: 741 of 2000.\n",
      "Step: 742 of 2000.\n",
      "Step: 743 of 2000.\n",
      "Step: 744 of 2000.\n",
      "Step: 745 of 2000.\n",
      "Step: 746 of 2000.\n",
      "Step: 747 of 2000.\n",
      "Step: 748 of 2000.\n",
      "Step: 749 of 2000.\n",
      "Step: 750 of 2000.\n",
      "Step: 751 of 2000.\n",
      "Step: 752 of 2000.\n",
      "Step: 753 of 2000.\n",
      "Step: 754 of 2000.\n",
      "Step: 755 of 2000.\n",
      "Step: 756 of 2000.\n",
      "Step: 757 of 2000.\n",
      "Step: 758 of 2000.\n",
      "Step: 759 of 2000.\n",
      "Saving batch of refined images at adversarial step: 759.\n",
      "Refiner model loss: [1.83965742 1.08220165 0.75745576].\n",
      "Discriminator model loss real: [0.36642579].\n",
      "Discriminator model loss refined: [0.31647272].\n",
      "Step: 760 of 2000.\n",
      "Step: 761 of 2000.\n",
      "Step: 762 of 2000.\n",
      "Step: 763 of 2000.\n",
      "Step: 764 of 2000.\n",
      "Step: 765 of 2000.\n",
      "Step: 766 of 2000.\n",
      "Step: 767 of 2000.\n",
      "Step: 768 of 2000.\n",
      "Step: 769 of 2000.\n",
      "Step: 770 of 2000.\n",
      "Step: 771 of 2000.\n",
      "Step: 772 of 2000.\n",
      "Step: 773 of 2000.\n",
      "Step: 774 of 2000.\n",
      "Step: 775 of 2000.\n",
      "Step: 776 of 2000.\n",
      "Step: 777 of 2000.\n",
      "Step: 778 of 2000.\n",
      "Step: 779 of 2000.\n",
      "Saving batch of refined images at adversarial step: 779.\n",
      "Refiner model loss: [1.8385743  1.07977323 0.75880107].\n",
      "Discriminator model loss real: [0.36656745].\n",
      "Discriminator model loss refined: [0.31568287].\n",
      "Step: 780 of 2000.\n",
      "Step: 781 of 2000.\n",
      "Step: 782 of 2000.\n",
      "Step: 783 of 2000.\n",
      "Step: 784 of 2000.\n",
      "Step: 785 of 2000.\n",
      "Step: 786 of 2000.\n",
      "Step: 787 of 2000.\n",
      "Step: 788 of 2000.\n",
      "Step: 789 of 2000.\n",
      "Step: 790 of 2000.\n",
      "Step: 791 of 2000.\n",
      "Step: 792 of 2000.\n",
      "Step: 793 of 2000.\n",
      "Step: 794 of 2000.\n",
      "Step: 795 of 2000.\n",
      "Step: 796 of 2000.\n",
      "Step: 797 of 2000.\n",
      "Step: 798 of 2000.\n",
      "Step: 799 of 2000.\n",
      "Saving batch of refined images at adversarial step: 799.\n",
      "Refiner model loss: [1.83033744 1.07006541 0.76027203].\n",
      "Discriminator model loss real: [0.36734255].\n",
      "Discriminator model loss refined: [0.31504076].\n",
      "Step: 800 of 2000.\n",
      "Step: 801 of 2000.\n",
      "Step: 802 of 2000.\n",
      "Step: 803 of 2000.\n",
      "Step: 804 of 2000.\n",
      "Step: 805 of 2000.\n",
      "Step: 806 of 2000.\n",
      "Step: 807 of 2000.\n",
      "Step: 808 of 2000.\n",
      "Step: 809 of 2000.\n",
      "Step: 810 of 2000.\n",
      "Step: 811 of 2000.\n",
      "Step: 812 of 2000.\n",
      "Step: 813 of 2000.\n",
      "Step: 814 of 2000.\n",
      "Step: 815 of 2000.\n",
      "Step: 816 of 2000.\n",
      "Step: 817 of 2000.\n",
      "Step: 818 of 2000.\n",
      "Step: 819 of 2000.\n",
      "Saving batch of refined images at adversarial step: 819.\n",
      "Refiner model loss: [1.82682829 1.06499873 0.76182956].\n",
      "Discriminator model loss real: [0.36738687].\n",
      "Discriminator model loss refined: [0.31443408].\n",
      "Step: 820 of 2000.\n",
      "Step: 821 of 2000.\n",
      "Step: 822 of 2000.\n",
      "Step: 823 of 2000.\n",
      "Step: 824 of 2000.\n",
      "Step: 825 of 2000.\n",
      "Step: 826 of 2000.\n",
      "Step: 827 of 2000.\n",
      "Step: 828 of 2000.\n",
      "Step: 829 of 2000.\n",
      "Step: 830 of 2000.\n",
      "Step: 831 of 2000.\n",
      "Step: 832 of 2000.\n",
      "Step: 833 of 2000.\n",
      "Step: 834 of 2000.\n",
      "Step: 835 of 2000.\n",
      "Step: 836 of 2000.\n",
      "Step: 837 of 2000.\n",
      "Step: 838 of 2000.\n",
      "Step: 839 of 2000.\n",
      "Saving batch of refined images at adversarial step: 839.\n",
      "Refiner model loss: [1.82384913 1.06043265 0.76341649].\n",
      "Discriminator model loss real: [0.36803655].\n",
      "Discriminator model loss refined: [0.31385584].\n",
      "Step: 840 of 2000.\n",
      "Step: 841 of 2000.\n",
      "Step: 842 of 2000.\n",
      "Step: 843 of 2000.\n",
      "Step: 844 of 2000.\n",
      "Step: 845 of 2000.\n",
      "Step: 846 of 2000.\n",
      "Step: 847 of 2000.\n",
      "Step: 848 of 2000.\n",
      "Step: 849 of 2000.\n",
      "Step: 850 of 2000.\n",
      "Step: 851 of 2000.\n",
      "Step: 852 of 2000.\n",
      "Step: 853 of 2000.\n",
      "Step: 854 of 2000.\n",
      "Step: 855 of 2000.\n",
      "Step: 856 of 2000.\n",
      "Step: 857 of 2000.\n",
      "Step: 858 of 2000.\n",
      "Step: 859 of 2000.\n",
      "Saving batch of refined images at adversarial step: 859.\n",
      "Refiner model loss: [1.8165757 1.0516693 0.7649064].\n",
      "Discriminator model loss real: [0.36845732].\n",
      "Discriminator model loss refined: [0.31304991].\n",
      "Step: 860 of 2000.\n",
      "Step: 861 of 2000.\n",
      "Step: 862 of 2000.\n",
      "Step: 863 of 2000.\n",
      "Step: 864 of 2000.\n",
      "Step: 865 of 2000.\n",
      "Step: 866 of 2000.\n",
      "Step: 867 of 2000.\n",
      "Step: 868 of 2000.\n",
      "Step: 869 of 2000.\n",
      "Step: 870 of 2000.\n",
      "Step: 871 of 2000.\n",
      "Step: 872 of 2000.\n",
      "Step: 873 of 2000.\n",
      "Step: 874 of 2000.\n",
      "Step: 875 of 2000.\n",
      "Step: 876 of 2000.\n",
      "Step: 877 of 2000.\n",
      "Step: 878 of 2000.\n",
      "Step: 879 of 2000.\n",
      "Saving batch of refined images at adversarial step: 879.\n",
      "Refiner model loss: [1.81596971 1.04952868 0.76644103].\n",
      "Discriminator model loss real: [0.36865006].\n",
      "Discriminator model loss refined: [0.31247455].\n",
      "Step: 880 of 2000.\n",
      "Step: 881 of 2000.\n",
      "Step: 882 of 2000.\n",
      "Step: 883 of 2000.\n",
      "Step: 884 of 2000.\n",
      "Step: 885 of 2000.\n",
      "Step: 886 of 2000.\n",
      "Step: 887 of 2000.\n",
      "Step: 888 of 2000.\n",
      "Step: 889 of 2000.\n",
      "Step: 890 of 2000.\n",
      "Step: 891 of 2000.\n",
      "Step: 892 of 2000.\n",
      "Step: 893 of 2000.\n",
      "Step: 894 of 2000.\n",
      "Step: 895 of 2000.\n",
      "Step: 896 of 2000.\n",
      "Step: 897 of 2000.\n",
      "Step: 898 of 2000.\n",
      "Step: 899 of 2000.\n",
      "Saving batch of refined images at adversarial step: 899.\n",
      "Refiner model loss: [1.81089645 1.04276    0.76813646].\n",
      "Discriminator model loss real: [0.36960254].\n",
      "Discriminator model loss refined: [0.31192796].\n",
      "Step: 900 of 2000.\n",
      "Step: 901 of 2000.\n",
      "Step: 902 of 2000.\n",
      "Step: 903 of 2000.\n",
      "Step: 904 of 2000.\n",
      "Step: 905 of 2000.\n",
      "Step: 906 of 2000.\n",
      "Step: 907 of 2000.\n",
      "Step: 908 of 2000.\n",
      "Step: 909 of 2000.\n",
      "Step: 910 of 2000.\n",
      "Step: 911 of 2000.\n",
      "Step: 912 of 2000.\n",
      "Step: 913 of 2000.\n",
      "Step: 914 of 2000.\n",
      "Step: 915 of 2000.\n",
      "Step: 916 of 2000.\n",
      "Step: 917 of 2000.\n",
      "Step: 918 of 2000.\n",
      "Step: 919 of 2000.\n",
      "Saving batch of refined images at adversarial step: 919.\n",
      "Refiner model loss: [1.80771731 1.03821891 0.7694984 ].\n",
      "Discriminator model loss real: [0.36972773].\n",
      "Discriminator model loss refined: [0.31129112].\n",
      "Step: 920 of 2000.\n",
      "Step: 921 of 2000.\n",
      "Step: 922 of 2000.\n",
      "Step: 923 of 2000.\n",
      "Step: 924 of 2000.\n",
      "Step: 925 of 2000.\n",
      "Step: 926 of 2000.\n",
      "Step: 927 of 2000.\n",
      "Step: 928 of 2000.\n",
      "Step: 929 of 2000.\n",
      "Step: 930 of 2000.\n",
      "Step: 931 of 2000.\n",
      "Step: 932 of 2000.\n",
      "Step: 933 of 2000.\n",
      "Step: 934 of 2000.\n",
      "Step: 935 of 2000.\n",
      "Step: 936 of 2000.\n",
      "Step: 937 of 2000.\n",
      "Step: 938 of 2000.\n",
      "Step: 939 of 2000.\n",
      "Saving batch of refined images at adversarial step: 939.\n",
      "Refiner model loss: [1.81034688 1.03916422 0.77118267].\n",
      "Discriminator model loss real: [0.37026293].\n",
      "Discriminator model loss refined: [0.31075406].\n",
      "Step: 940 of 2000.\n",
      "Step: 941 of 2000.\n",
      "Step: 942 of 2000.\n",
      "Step: 943 of 2000.\n",
      "Step: 944 of 2000.\n",
      "Step: 945 of 2000.\n",
      "Step: 946 of 2000.\n",
      "Step: 947 of 2000.\n",
      "Step: 948 of 2000.\n",
      "Step: 949 of 2000.\n",
      "Step: 950 of 2000.\n",
      "Step: 951 of 2000.\n",
      "Step: 952 of 2000.\n",
      "Step: 953 of 2000.\n",
      "Step: 954 of 2000.\n",
      "Step: 955 of 2000.\n",
      "Step: 956 of 2000.\n",
      "Step: 957 of 2000.\n",
      "Step: 958 of 2000.\n",
      "Step: 959 of 2000.\n",
      "Saving batch of refined images at adversarial step: 959.\n",
      "Refiner model loss: [1.80468645 1.03216583 0.77252062].\n",
      "Discriminator model loss real: [0.37049065].\n",
      "Discriminator model loss refined: [0.30995901].\n",
      "Step: 960 of 2000.\n",
      "Step: 961 of 2000.\n",
      "Step: 962 of 2000.\n",
      "Step: 963 of 2000.\n",
      "Step: 964 of 2000.\n",
      "Step: 965 of 2000.\n",
      "Step: 966 of 2000.\n",
      "Step: 967 of 2000.\n",
      "Step: 968 of 2000.\n",
      "Step: 969 of 2000.\n",
      "Step: 970 of 2000.\n",
      "Step: 971 of 2000.\n",
      "Step: 972 of 2000.\n",
      "Step: 973 of 2000.\n",
      "Step: 974 of 2000.\n",
      "Step: 975 of 2000.\n",
      "Step: 976 of 2000.\n",
      "Step: 977 of 2000.\n",
      "Step: 978 of 2000.\n",
      "Step: 979 of 2000.\n",
      "Saving batch of refined images at adversarial step: 979.\n",
      "Refiner model loss: [1.80009871 1.02583208 0.77426663].\n",
      "Discriminator model loss real: [0.37114963].\n",
      "Discriminator model loss refined: [0.30944111].\n",
      "Step: 980 of 2000.\n",
      "Step: 981 of 2000.\n",
      "Step: 982 of 2000.\n",
      "Step: 983 of 2000.\n",
      "Step: 984 of 2000.\n",
      "Step: 985 of 2000.\n",
      "Step: 986 of 2000.\n",
      "Step: 987 of 2000.\n",
      "Step: 988 of 2000.\n",
      "Step: 989 of 2000.\n",
      "Step: 990 of 2000.\n",
      "Step: 991 of 2000.\n",
      "Step: 992 of 2000.\n",
      "Step: 993 of 2000.\n",
      "Step: 994 of 2000.\n",
      "Step: 995 of 2000.\n",
      "Step: 996 of 2000.\n",
      "Step: 997 of 2000.\n",
      "Step: 998 of 2000.\n",
      "Step: 999 of 2000.\n",
      "Saving batch of refined images at adversarial step: 999.\n",
      "Refiner model loss: [1.80002508 1.02406738 0.77595769].\n",
      "Discriminator model loss real: [0.37156968].\n",
      "Discriminator model loss refined: [0.30888351].\n",
      "Step: 1000 of 2000.\n",
      "Step: 1001 of 2000.\n",
      "Step: 1002 of 2000.\n",
      "Step: 1003 of 2000.\n",
      "Step: 1004 of 2000.\n",
      "Step: 1005 of 2000.\n",
      "Step: 1006 of 2000.\n",
      "Step: 1007 of 2000.\n",
      "Step: 1008 of 2000.\n",
      "Step: 1009 of 2000.\n",
      "Step: 1010 of 2000.\n",
      "Step: 1011 of 2000.\n",
      "Step: 1012 of 2000.\n",
      "Step: 1013 of 2000.\n",
      "Step: 1014 of 2000.\n",
      "Step: 1015 of 2000.\n",
      "Step: 1016 of 2000.\n",
      "Step: 1017 of 2000.\n",
      "Step: 1018 of 2000.\n",
      "Step: 1019 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1019.\n",
      "Refiner model loss: [1.79508915 1.01770118 0.77738796].\n",
      "Discriminator model loss real: [0.37208928].\n",
      "Discriminator model loss refined: [0.30807965].\n",
      "Step: 1020 of 2000.\n",
      "Step: 1021 of 2000.\n",
      "Step: 1022 of 2000.\n",
      "Step: 1023 of 2000.\n",
      "Step: 1024 of 2000.\n",
      "Step: 1025 of 2000.\n",
      "Step: 1026 of 2000.\n",
      "Step: 1027 of 2000.\n",
      "Step: 1028 of 2000.\n",
      "Step: 1029 of 2000.\n",
      "Step: 1030 of 2000.\n",
      "Step: 1031 of 2000.\n",
      "Step: 1032 of 2000.\n",
      "Step: 1033 of 2000.\n",
      "Step: 1034 of 2000.\n",
      "Step: 1035 of 2000.\n",
      "Step: 1036 of 2000.\n",
      "Step: 1037 of 2000.\n",
      "Step: 1038 of 2000.\n",
      "Step: 1039 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1039.\n",
      "Refiner model loss: [1.79282514 1.01395889 0.77886626].\n",
      "Discriminator model loss real: [0.37235312].\n",
      "Discriminator model loss refined: [0.30721814].\n",
      "Step: 1040 of 2000.\n",
      "Step: 1041 of 2000.\n",
      "Step: 1042 of 2000.\n",
      "Step: 1043 of 2000.\n",
      "Step: 1044 of 2000.\n",
      "Step: 1045 of 2000.\n",
      "Step: 1046 of 2000.\n",
      "Step: 1047 of 2000.\n",
      "Step: 1048 of 2000.\n",
      "Step: 1049 of 2000.\n",
      "Step: 1050 of 2000.\n",
      "Step: 1051 of 2000.\n",
      "Step: 1052 of 2000.\n",
      "Step: 1053 of 2000.\n",
      "Step: 1054 of 2000.\n",
      "Step: 1055 of 2000.\n",
      "Step: 1056 of 2000.\n",
      "Step: 1057 of 2000.\n",
      "Step: 1058 of 2000.\n",
      "Step: 1059 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1059.\n",
      "Refiner model loss: [1.7862799  1.00563234 0.78064755].\n",
      "Discriminator model loss real: [0.37289839].\n",
      "Discriminator model loss refined: [0.30662597].\n",
      "Step: 1060 of 2000.\n",
      "Step: 1061 of 2000.\n",
      "Step: 1062 of 2000.\n",
      "Step: 1063 of 2000.\n",
      "Step: 1064 of 2000.\n",
      "Step: 1065 of 2000.\n",
      "Step: 1066 of 2000.\n",
      "Step: 1067 of 2000.\n",
      "Step: 1068 of 2000.\n",
      "Step: 1069 of 2000.\n",
      "Step: 1070 of 2000.\n",
      "Step: 1071 of 2000.\n",
      "Step: 1072 of 2000.\n",
      "Step: 1073 of 2000.\n",
      "Step: 1074 of 2000.\n",
      "Step: 1075 of 2000.\n",
      "Step: 1076 of 2000.\n",
      "Step: 1077 of 2000.\n",
      "Step: 1078 of 2000.\n",
      "Step: 1079 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1079.\n",
      "Refiner model loss: [1.7860188  1.00387691 0.78214189].\n",
      "Discriminator model loss real: [0.37344159].\n",
      "Discriminator model loss refined: [0.30600141].\n",
      "Step: 1080 of 2000.\n",
      "Step: 1081 of 2000.\n",
      "Step: 1082 of 2000.\n",
      "Step: 1083 of 2000.\n",
      "Step: 1084 of 2000.\n",
      "Step: 1085 of 2000.\n",
      "Step: 1086 of 2000.\n",
      "Step: 1087 of 2000.\n",
      "Step: 1088 of 2000.\n",
      "Step: 1089 of 2000.\n",
      "Step: 1090 of 2000.\n",
      "Step: 1091 of 2000.\n",
      "Step: 1092 of 2000.\n",
      "Step: 1093 of 2000.\n",
      "Step: 1094 of 2000.\n",
      "Step: 1095 of 2000.\n",
      "Step: 1096 of 2000.\n",
      "Step: 1097 of 2000.\n",
      "Step: 1098 of 2000.\n",
      "Step: 1099 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1099.\n",
      "Refiner model loss: [1.78385898 1.00007082 0.78378817].\n",
      "Discriminator model loss real: [0.37369319].\n",
      "Discriminator model loss refined: [0.30551452].\n",
      "Step: 1100 of 2000.\n",
      "Step: 1101 of 2000.\n",
      "Step: 1102 of 2000.\n",
      "Step: 1103 of 2000.\n",
      "Step: 1104 of 2000.\n",
      "Step: 1105 of 2000.\n",
      "Step: 1106 of 2000.\n",
      "Step: 1107 of 2000.\n",
      "Step: 1108 of 2000.\n",
      "Step: 1109 of 2000.\n",
      "Step: 1110 of 2000.\n",
      "Step: 1111 of 2000.\n",
      "Step: 1112 of 2000.\n",
      "Step: 1113 of 2000.\n",
      "Step: 1114 of 2000.\n",
      "Step: 1115 of 2000.\n",
      "Step: 1116 of 2000.\n",
      "Step: 1117 of 2000.\n",
      "Step: 1118 of 2000.\n",
      "Step: 1119 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1119.\n",
      "Refiner model loss: [1.78262078 0.99740266 0.78521812].\n",
      "Discriminator model loss real: [0.37412811].\n",
      "Discriminator model loss refined: [0.30463149].\n",
      "Step: 1120 of 2000.\n",
      "Step: 1121 of 2000.\n",
      "Step: 1122 of 2000.\n",
      "Step: 1123 of 2000.\n",
      "Step: 1124 of 2000.\n",
      "Step: 1125 of 2000.\n",
      "Step: 1126 of 2000.\n",
      "Step: 1127 of 2000.\n",
      "Step: 1128 of 2000.\n",
      "Step: 1129 of 2000.\n",
      "Step: 1130 of 2000.\n",
      "Step: 1131 of 2000.\n",
      "Step: 1132 of 2000.\n",
      "Step: 1133 of 2000.\n",
      "Step: 1134 of 2000.\n",
      "Step: 1135 of 2000.\n",
      "Step: 1136 of 2000.\n",
      "Step: 1137 of 2000.\n",
      "Step: 1138 of 2000.\n",
      "Step: 1139 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1139.\n",
      "Refiner model loss: [1.77976254 0.9929741  0.78678845].\n",
      "Discriminator model loss real: [0.37484834].\n",
      "Discriminator model loss refined: [0.30395165].\n",
      "Step: 1140 of 2000.\n",
      "Step: 1141 of 2000.\n",
      "Step: 1142 of 2000.\n",
      "Step: 1143 of 2000.\n",
      "Step: 1144 of 2000.\n",
      "Step: 1145 of 2000.\n",
      "Step: 1146 of 2000.\n",
      "Step: 1147 of 2000.\n",
      "Step: 1148 of 2000.\n",
      "Step: 1149 of 2000.\n",
      "Step: 1150 of 2000.\n",
      "Step: 1151 of 2000.\n",
      "Step: 1152 of 2000.\n",
      "Step: 1153 of 2000.\n",
      "Step: 1154 of 2000.\n",
      "Step: 1155 of 2000.\n",
      "Step: 1156 of 2000.\n",
      "Step: 1157 of 2000.\n",
      "Step: 1158 of 2000.\n",
      "Step: 1159 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1159.\n",
      "Refiner model loss: [1.78047851 0.99212052 0.78835799].\n",
      "Discriminator model loss real: [0.37501181].\n",
      "Discriminator model loss refined: [0.30344204].\n",
      "Step: 1160 of 2000.\n",
      "Step: 1161 of 2000.\n",
      "Step: 1162 of 2000.\n",
      "Step: 1163 of 2000.\n",
      "Step: 1164 of 2000.\n",
      "Step: 1165 of 2000.\n",
      "Step: 1166 of 2000.\n",
      "Step: 1167 of 2000.\n",
      "Step: 1168 of 2000.\n",
      "Step: 1169 of 2000.\n",
      "Step: 1170 of 2000.\n",
      "Step: 1171 of 2000.\n",
      "Step: 1172 of 2000.\n",
      "Step: 1173 of 2000.\n",
      "Step: 1174 of 2000.\n",
      "Step: 1175 of 2000.\n",
      "Step: 1176 of 2000.\n",
      "Step: 1177 of 2000.\n",
      "Step: 1178 of 2000.\n",
      "Step: 1179 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1179.\n",
      "Refiner model loss: [1.77820105 0.98813993 0.79006113].\n",
      "Discriminator model loss real: [0.37572723].\n",
      "Discriminator model loss refined: [0.30285617].\n",
      "Step: 1180 of 2000.\n",
      "Step: 1181 of 2000.\n",
      "Step: 1182 of 2000.\n",
      "Step: 1183 of 2000.\n",
      "Step: 1184 of 2000.\n",
      "Step: 1185 of 2000.\n",
      "Step: 1186 of 2000.\n",
      "Step: 1187 of 2000.\n",
      "Step: 1188 of 2000.\n",
      "Step: 1189 of 2000.\n",
      "Step: 1190 of 2000.\n",
      "Step: 1191 of 2000.\n",
      "Step: 1192 of 2000.\n",
      "Step: 1193 of 2000.\n",
      "Step: 1194 of 2000.\n",
      "Step: 1195 of 2000.\n",
      "Step: 1196 of 2000.\n",
      "Step: 1197 of 2000.\n",
      "Step: 1198 of 2000.\n",
      "Step: 1199 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1199.\n",
      "Refiner model loss: [1.77663244 0.98509376 0.79153868].\n",
      "Discriminator model loss real: [0.37610491].\n",
      "Discriminator model loss refined: [0.3022238].\n",
      "Step: 1200 of 2000.\n",
      "Step: 1201 of 2000.\n",
      "Step: 1202 of 2000.\n",
      "Step: 1203 of 2000.\n",
      "Step: 1204 of 2000.\n",
      "Step: 1205 of 2000.\n",
      "Step: 1206 of 2000.\n",
      "Step: 1207 of 2000.\n",
      "Step: 1208 of 2000.\n",
      "Step: 1209 of 2000.\n",
      "Step: 1210 of 2000.\n",
      "Step: 1211 of 2000.\n",
      "Step: 1212 of 2000.\n",
      "Step: 1213 of 2000.\n",
      "Step: 1214 of 2000.\n",
      "Step: 1215 of 2000.\n",
      "Step: 1216 of 2000.\n",
      "Step: 1217 of 2000.\n",
      "Step: 1218 of 2000.\n",
      "Step: 1219 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1219.\n",
      "Refiner model loss: [1.77540642 0.98216111 0.79324532].\n",
      "Discriminator model loss real: [0.376296].\n",
      "Discriminator model loss refined: [0.30187373].\n",
      "Step: 1220 of 2000.\n",
      "Step: 1221 of 2000.\n",
      "Step: 1222 of 2000.\n",
      "Step: 1223 of 2000.\n",
      "Step: 1224 of 2000.\n",
      "Step: 1225 of 2000.\n",
      "Step: 1226 of 2000.\n",
      "Step: 1227 of 2000.\n",
      "Step: 1228 of 2000.\n",
      "Step: 1229 of 2000.\n",
      "Step: 1230 of 2000.\n",
      "Step: 1231 of 2000.\n",
      "Step: 1232 of 2000.\n",
      "Step: 1233 of 2000.\n",
      "Step: 1234 of 2000.\n",
      "Step: 1235 of 2000.\n",
      "Step: 1236 of 2000.\n",
      "Step: 1237 of 2000.\n",
      "Step: 1238 of 2000.\n",
      "Step: 1239 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1239.\n",
      "Refiner model loss: [1.77138623 0.97683383 0.79455241].\n",
      "Discriminator model loss real: [0.37680874].\n",
      "Discriminator model loss refined: [0.30103159].\n",
      "Step: 1240 of 2000.\n",
      "Step: 1241 of 2000.\n",
      "Step: 1242 of 2000.\n",
      "Step: 1243 of 2000.\n",
      "Step: 1244 of 2000.\n",
      "Step: 1245 of 2000.\n",
      "Step: 1246 of 2000.\n",
      "Step: 1247 of 2000.\n",
      "Step: 1248 of 2000.\n",
      "Step: 1249 of 2000.\n",
      "Step: 1250 of 2000.\n",
      "Step: 1251 of 2000.\n",
      "Step: 1252 of 2000.\n",
      "Step: 1253 of 2000.\n",
      "Step: 1254 of 2000.\n",
      "Step: 1255 of 2000.\n",
      "Step: 1256 of 2000.\n",
      "Step: 1257 of 2000.\n",
      "Step: 1258 of 2000.\n",
      "Step: 1259 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1259.\n",
      "Refiner model loss: [1.76985409 0.97389154 0.79596255].\n",
      "Discriminator model loss real: [0.37741534].\n",
      "Discriminator model loss refined: [0.30039888].\n",
      "Step: 1260 of 2000.\n",
      "Step: 1261 of 2000.\n",
      "Step: 1262 of 2000.\n",
      "Step: 1263 of 2000.\n",
      "Step: 1264 of 2000.\n",
      "Step: 1265 of 2000.\n",
      "Step: 1266 of 2000.\n",
      "Step: 1267 of 2000.\n",
      "Step: 1268 of 2000.\n",
      "Step: 1269 of 2000.\n",
      "Step: 1270 of 2000.\n",
      "Step: 1271 of 2000.\n",
      "Step: 1272 of 2000.\n",
      "Step: 1273 of 2000.\n",
      "Step: 1274 of 2000.\n",
      "Step: 1275 of 2000.\n",
      "Step: 1276 of 2000.\n",
      "Step: 1277 of 2000.\n",
      "Step: 1278 of 2000.\n",
      "Step: 1279 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1279.\n",
      "Refiner model loss: [1.77118077 0.97357257 0.7976082 ].\n",
      "Discriminator model loss real: [0.37761557].\n",
      "Discriminator model loss refined: [0.29995827].\n",
      "Step: 1280 of 2000.\n",
      "Step: 1281 of 2000.\n",
      "Step: 1282 of 2000.\n",
      "Step: 1283 of 2000.\n",
      "Step: 1284 of 2000.\n",
      "Step: 1285 of 2000.\n",
      "Step: 1286 of 2000.\n",
      "Step: 1287 of 2000.\n",
      "Step: 1288 of 2000.\n",
      "Step: 1289 of 2000.\n",
      "Step: 1290 of 2000.\n",
      "Step: 1291 of 2000.\n",
      "Step: 1292 of 2000.\n",
      "Step: 1293 of 2000.\n",
      "Step: 1294 of 2000.\n",
      "Step: 1295 of 2000.\n",
      "Step: 1296 of 2000.\n",
      "Step: 1297 of 2000.\n",
      "Step: 1298 of 2000.\n",
      "Step: 1299 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1299.\n",
      "Refiner model loss: [1.76753518 0.96856414 0.79897105].\n",
      "Discriminator model loss real: [0.37810389].\n",
      "Discriminator model loss refined: [0.29932808].\n",
      "Step: 1300 of 2000.\n",
      "Step: 1301 of 2000.\n",
      "Step: 1302 of 2000.\n",
      "Step: 1303 of 2000.\n",
      "Step: 1304 of 2000.\n",
      "Step: 1305 of 2000.\n",
      "Step: 1306 of 2000.\n",
      "Step: 1307 of 2000.\n",
      "Step: 1308 of 2000.\n",
      "Step: 1309 of 2000.\n",
      "Step: 1310 of 2000.\n",
      "Step: 1311 of 2000.\n",
      "Step: 1312 of 2000.\n",
      "Step: 1313 of 2000.\n",
      "Step: 1314 of 2000.\n",
      "Step: 1315 of 2000.\n",
      "Step: 1316 of 2000.\n",
      "Step: 1317 of 2000.\n",
      "Step: 1318 of 2000.\n",
      "Step: 1319 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1319.\n",
      "Refiner model loss: [1.76455381 0.96406475 0.80048907].\n",
      "Discriminator model loss real: [0.37862353].\n",
      "Discriminator model loss refined: [0.29897337].\n",
      "Step: 1320 of 2000.\n",
      "Step: 1321 of 2000.\n",
      "Step: 1322 of 2000.\n",
      "Step: 1323 of 2000.\n",
      "Step: 1324 of 2000.\n",
      "Step: 1325 of 2000.\n",
      "Step: 1326 of 2000.\n",
      "Step: 1327 of 2000.\n",
      "Step: 1328 of 2000.\n",
      "Step: 1329 of 2000.\n",
      "Step: 1330 of 2000.\n",
      "Step: 1331 of 2000.\n",
      "Step: 1332 of 2000.\n",
      "Step: 1333 of 2000.\n",
      "Step: 1334 of 2000.\n",
      "Step: 1335 of 2000.\n",
      "Step: 1336 of 2000.\n",
      "Step: 1337 of 2000.\n",
      "Step: 1338 of 2000.\n",
      "Step: 1339 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1339.\n",
      "Refiner model loss: [1.76679414 0.9649648  0.80182934].\n",
      "Discriminator model loss real: [0.37886727].\n",
      "Discriminator model loss refined: [0.2982831].\n",
      "Step: 1340 of 2000.\n",
      "Step: 1341 of 2000.\n",
      "Step: 1342 of 2000.\n",
      "Step: 1343 of 2000.\n",
      "Step: 1344 of 2000.\n",
      "Step: 1345 of 2000.\n",
      "Step: 1346 of 2000.\n",
      "Step: 1347 of 2000.\n",
      "Step: 1348 of 2000.\n",
      "Step: 1349 of 2000.\n",
      "Step: 1350 of 2000.\n",
      "Step: 1351 of 2000.\n",
      "Step: 1352 of 2000.\n",
      "Step: 1353 of 2000.\n",
      "Step: 1354 of 2000.\n",
      "Step: 1355 of 2000.\n",
      "Step: 1356 of 2000.\n",
      "Step: 1357 of 2000.\n",
      "Step: 1358 of 2000.\n",
      "Step: 1359 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1359.\n",
      "Refiner model loss: [1.76703342 0.96376736 0.80326606].\n",
      "Discriminator model loss real: [0.37918209].\n",
      "Discriminator model loss refined: [0.29763449].\n",
      "Step: 1360 of 2000.\n",
      "Step: 1361 of 2000.\n",
      "Step: 1362 of 2000.\n",
      "Step: 1363 of 2000.\n",
      "Step: 1364 of 2000.\n",
      "Step: 1365 of 2000.\n",
      "Step: 1366 of 2000.\n",
      "Step: 1367 of 2000.\n",
      "Step: 1368 of 2000.\n",
      "Step: 1369 of 2000.\n",
      "Step: 1370 of 2000.\n",
      "Step: 1371 of 2000.\n",
      "Step: 1372 of 2000.\n",
      "Step: 1373 of 2000.\n",
      "Step: 1374 of 2000.\n",
      "Step: 1375 of 2000.\n",
      "Step: 1376 of 2000.\n",
      "Step: 1377 of 2000.\n",
      "Step: 1378 of 2000.\n",
      "Step: 1379 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1379.\n",
      "Refiner model loss: [1.7600357  0.95523024 0.80480545].\n",
      "Discriminator model loss real: [0.37981289].\n",
      "Discriminator model loss refined: [0.29721997].\n",
      "Step: 1380 of 2000.\n",
      "Step: 1381 of 2000.\n",
      "Step: 1382 of 2000.\n",
      "Step: 1383 of 2000.\n",
      "Step: 1384 of 2000.\n",
      "Step: 1385 of 2000.\n",
      "Step: 1386 of 2000.\n",
      "Step: 1387 of 2000.\n",
      "Step: 1388 of 2000.\n",
      "Step: 1389 of 2000.\n",
      "Step: 1390 of 2000.\n",
      "Step: 1391 of 2000.\n",
      "Step: 1392 of 2000.\n",
      "Step: 1393 of 2000.\n",
      "Step: 1394 of 2000.\n",
      "Step: 1395 of 2000.\n",
      "Step: 1396 of 2000.\n",
      "Step: 1397 of 2000.\n",
      "Step: 1398 of 2000.\n",
      "Step: 1399 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1399.\n",
      "Refiner model loss: [1.76374169 0.95767264 0.80606906].\n",
      "Discriminator model loss real: [0.38005813].\n",
      "Discriminator model loss refined: [0.29686946].\n",
      "Step: 1400 of 2000.\n",
      "Step: 1401 of 2000.\n",
      "Step: 1402 of 2000.\n",
      "Step: 1403 of 2000.\n",
      "Step: 1404 of 2000.\n",
      "Step: 1405 of 2000.\n",
      "Step: 1406 of 2000.\n",
      "Step: 1407 of 2000.\n",
      "Step: 1408 of 2000.\n",
      "Step: 1409 of 2000.\n",
      "Step: 1410 of 2000.\n",
      "Step: 1411 of 2000.\n",
      "Step: 1412 of 2000.\n",
      "Step: 1413 of 2000.\n",
      "Step: 1414 of 2000.\n",
      "Step: 1415 of 2000.\n",
      "Step: 1416 of 2000.\n",
      "Step: 1417 of 2000.\n",
      "Step: 1418 of 2000.\n",
      "Step: 1419 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1419.\n",
      "Refiner model loss: [1.76072569 0.95338434 0.80734135].\n",
      "Discriminator model loss real: [0.38038232].\n",
      "Discriminator model loss refined: [0.29628244].\n",
      "Step: 1420 of 2000.\n",
      "Step: 1421 of 2000.\n",
      "Step: 1422 of 2000.\n",
      "Step: 1423 of 2000.\n",
      "Step: 1424 of 2000.\n",
      "Step: 1425 of 2000.\n",
      "Step: 1426 of 2000.\n",
      "Step: 1427 of 2000.\n",
      "Step: 1428 of 2000.\n",
      "Step: 1429 of 2000.\n",
      "Step: 1430 of 2000.\n",
      "Step: 1431 of 2000.\n",
      "Step: 1432 of 2000.\n",
      "Step: 1433 of 2000.\n",
      "Step: 1434 of 2000.\n",
      "Step: 1435 of 2000.\n",
      "Step: 1436 of 2000.\n",
      "Step: 1437 of 2000.\n",
      "Step: 1438 of 2000.\n",
      "Step: 1439 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1439.\n",
      "Refiner model loss: [1.75819993 0.94948703 0.8087129 ].\n",
      "Discriminator model loss real: [0.3806893].\n",
      "Discriminator model loss refined: [0.29581711].\n",
      "Step: 1440 of 2000.\n",
      "Step: 1441 of 2000.\n",
      "Step: 1442 of 2000.\n",
      "Step: 1443 of 2000.\n",
      "Step: 1444 of 2000.\n",
      "Step: 1445 of 2000.\n",
      "Step: 1446 of 2000.\n",
      "Step: 1447 of 2000.\n",
      "Step: 1448 of 2000.\n",
      "Step: 1449 of 2000.\n",
      "Step: 1450 of 2000.\n",
      "Step: 1451 of 2000.\n",
      "Step: 1452 of 2000.\n",
      "Step: 1453 of 2000.\n",
      "Step: 1454 of 2000.\n",
      "Step: 1455 of 2000.\n",
      "Step: 1456 of 2000.\n",
      "Step: 1457 of 2000.\n",
      "Step: 1458 of 2000.\n",
      "Step: 1459 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1459.\n",
      "Refiner model loss: [1.7523067  0.94207589 0.81023081].\n",
      "Discriminator model loss real: [0.38127588].\n",
      "Discriminator model loss refined: [0.29517905].\n",
      "Step: 1460 of 2000.\n",
      "Step: 1461 of 2000.\n",
      "Step: 1462 of 2000.\n",
      "Step: 1463 of 2000.\n",
      "Step: 1464 of 2000.\n",
      "Step: 1465 of 2000.\n",
      "Step: 1466 of 2000.\n",
      "Step: 1467 of 2000.\n",
      "Step: 1468 of 2000.\n",
      "Step: 1469 of 2000.\n",
      "Step: 1470 of 2000.\n",
      "Step: 1471 of 2000.\n",
      "Step: 1472 of 2000.\n",
      "Step: 1473 of 2000.\n",
      "Step: 1474 of 2000.\n",
      "Step: 1475 of 2000.\n",
      "Step: 1476 of 2000.\n",
      "Step: 1477 of 2000.\n",
      "Step: 1478 of 2000.\n",
      "Step: 1479 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1479.\n",
      "Refiner model loss: [1.7545856  0.94297876 0.81160684].\n",
      "Discriminator model loss real: [0.38155972].\n",
      "Discriminator model loss refined: [0.29497634].\n",
      "Step: 1480 of 2000.\n",
      "Step: 1481 of 2000.\n",
      "Step: 1482 of 2000.\n",
      "Step: 1483 of 2000.\n",
      "Step: 1484 of 2000.\n",
      "Step: 1485 of 2000.\n",
      "Step: 1486 of 2000.\n",
      "Step: 1487 of 2000.\n",
      "Step: 1488 of 2000.\n",
      "Step: 1489 of 2000.\n",
      "Step: 1490 of 2000.\n",
      "Step: 1491 of 2000.\n",
      "Step: 1492 of 2000.\n",
      "Step: 1493 of 2000.\n",
      "Step: 1494 of 2000.\n",
      "Step: 1495 of 2000.\n",
      "Step: 1496 of 2000.\n",
      "Step: 1497 of 2000.\n",
      "Step: 1498 of 2000.\n",
      "Step: 1499 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1499.\n",
      "Refiner model loss: [1.75336113 0.94075087 0.81261026].\n",
      "Discriminator model loss real: [0.38203201].\n",
      "Discriminator model loss refined: [0.294222].\n",
      "Step: 1500 of 2000.\n",
      "Step: 1501 of 2000.\n",
      "Step: 1502 of 2000.\n",
      "Step: 1503 of 2000.\n",
      "Step: 1504 of 2000.\n",
      "Step: 1505 of 2000.\n",
      "Step: 1506 of 2000.\n",
      "Step: 1507 of 2000.\n",
      "Step: 1508 of 2000.\n",
      "Step: 1509 of 2000.\n",
      "Step: 1510 of 2000.\n",
      "Step: 1511 of 2000.\n",
      "Step: 1512 of 2000.\n",
      "Step: 1513 of 2000.\n",
      "Step: 1514 of 2000.\n",
      "Step: 1515 of 2000.\n",
      "Step: 1516 of 2000.\n",
      "Step: 1517 of 2000.\n",
      "Step: 1518 of 2000.\n",
      "Step: 1519 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1519.\n",
      "Refiner model loss: [1.75133461 0.93735629 0.81397832].\n",
      "Discriminator model loss real: [0.38192696].\n",
      "Discriminator model loss refined: [0.29382966].\n",
      "Step: 1520 of 2000.\n",
      "Step: 1521 of 2000.\n",
      "Step: 1522 of 2000.\n",
      "Step: 1523 of 2000.\n",
      "Step: 1524 of 2000.\n",
      "Step: 1525 of 2000.\n",
      "Step: 1526 of 2000.\n",
      "Step: 1527 of 2000.\n",
      "Step: 1528 of 2000.\n",
      "Step: 1529 of 2000.\n",
      "Step: 1530 of 2000.\n",
      "Step: 1531 of 2000.\n",
      "Step: 1532 of 2000.\n",
      "Step: 1533 of 2000.\n",
      "Step: 1534 of 2000.\n",
      "Step: 1535 of 2000.\n",
      "Step: 1536 of 2000.\n",
      "Step: 1537 of 2000.\n",
      "Step: 1538 of 2000.\n",
      "Step: 1539 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1539.\n",
      "Refiner model loss: [1.75277883 0.93748861 0.81529023].\n",
      "Discriminator model loss real: [0.3824495].\n",
      "Discriminator model loss refined: [0.29357438].\n",
      "Step: 1540 of 2000.\n",
      "Step: 1541 of 2000.\n",
      "Step: 1542 of 2000.\n",
      "Step: 1543 of 2000.\n",
      "Step: 1544 of 2000.\n",
      "Step: 1545 of 2000.\n",
      "Step: 1546 of 2000.\n",
      "Step: 1547 of 2000.\n",
      "Step: 1548 of 2000.\n",
      "Step: 1549 of 2000.\n",
      "Step: 1550 of 2000.\n",
      "Step: 1551 of 2000.\n",
      "Step: 1552 of 2000.\n",
      "Step: 1553 of 2000.\n",
      "Step: 1554 of 2000.\n",
      "Step: 1555 of 2000.\n",
      "Step: 1556 of 2000.\n",
      "Step: 1557 of 2000.\n",
      "Step: 1558 of 2000.\n",
      "Step: 1559 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1559.\n",
      "Refiner model loss: [1.75424358 0.93779446 0.81644913].\n",
      "Discriminator model loss real: [0.38300867].\n",
      "Discriminator model loss refined: [0.29278184].\n",
      "Step: 1560 of 2000.\n",
      "Step: 1561 of 2000.\n",
      "Step: 1562 of 2000.\n",
      "Step: 1563 of 2000.\n",
      "Step: 1564 of 2000.\n",
      "Step: 1565 of 2000.\n",
      "Step: 1566 of 2000.\n",
      "Step: 1567 of 2000.\n",
      "Step: 1568 of 2000.\n",
      "Step: 1569 of 2000.\n",
      "Step: 1570 of 2000.\n",
      "Step: 1571 of 2000.\n",
      "Step: 1572 of 2000.\n",
      "Step: 1573 of 2000.\n",
      "Step: 1574 of 2000.\n",
      "Step: 1575 of 2000.\n",
      "Step: 1576 of 2000.\n",
      "Step: 1577 of 2000.\n",
      "Step: 1578 of 2000.\n",
      "Step: 1579 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1579.\n",
      "Refiner model loss: [1.75240917 0.93474971 0.81765946].\n",
      "Discriminator model loss real: [0.3831555].\n",
      "Discriminator model loss refined: [0.29249076].\n",
      "Step: 1580 of 2000.\n",
      "Step: 1581 of 2000.\n",
      "Step: 1582 of 2000.\n",
      "Step: 1583 of 2000.\n",
      "Step: 1584 of 2000.\n",
      "Step: 1585 of 2000.\n",
      "Step: 1586 of 2000.\n",
      "Step: 1587 of 2000.\n",
      "Step: 1588 of 2000.\n",
      "Step: 1589 of 2000.\n",
      "Step: 1590 of 2000.\n",
      "Step: 1591 of 2000.\n",
      "Step: 1592 of 2000.\n",
      "Step: 1593 of 2000.\n",
      "Step: 1594 of 2000.\n",
      "Step: 1595 of 2000.\n",
      "Step: 1596 of 2000.\n",
      "Step: 1597 of 2000.\n",
      "Step: 1598 of 2000.\n",
      "Step: 1599 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1599.\n",
      "Refiner model loss: [1.75223201 0.93360894 0.81862307].\n",
      "Discriminator model loss real: [0.38363757].\n",
      "Discriminator model loss refined: [0.29199381].\n",
      "Step: 1600 of 2000.\n",
      "Step: 1601 of 2000.\n",
      "Step: 1602 of 2000.\n",
      "Step: 1603 of 2000.\n",
      "Step: 1604 of 2000.\n",
      "Step: 1605 of 2000.\n",
      "Step: 1606 of 2000.\n",
      "Step: 1607 of 2000.\n",
      "Step: 1608 of 2000.\n",
      "Step: 1609 of 2000.\n",
      "Step: 1610 of 2000.\n",
      "Step: 1611 of 2000.\n",
      "Step: 1612 of 2000.\n",
      "Step: 1613 of 2000.\n",
      "Step: 1614 of 2000.\n",
      "Step: 1615 of 2000.\n",
      "Step: 1616 of 2000.\n",
      "Step: 1617 of 2000.\n",
      "Step: 1618 of 2000.\n",
      "Step: 1619 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1619.\n",
      "Refiner model loss: [1.74857707 0.92871166 0.81986541].\n",
      "Discriminator model loss real: [0.38367587].\n",
      "Discriminator model loss refined: [0.29144509].\n",
      "Step: 1620 of 2000.\n",
      "Step: 1621 of 2000.\n",
      "Step: 1622 of 2000.\n",
      "Step: 1623 of 2000.\n",
      "Step: 1624 of 2000.\n",
      "Step: 1625 of 2000.\n",
      "Step: 1626 of 2000.\n",
      "Step: 1627 of 2000.\n",
      "Step: 1628 of 2000.\n",
      "Step: 1629 of 2000.\n",
      "Step: 1630 of 2000.\n",
      "Step: 1631 of 2000.\n",
      "Step: 1632 of 2000.\n",
      "Step: 1633 of 2000.\n",
      "Step: 1634 of 2000.\n",
      "Step: 1635 of 2000.\n",
      "Step: 1636 of 2000.\n",
      "Step: 1637 of 2000.\n",
      "Step: 1638 of 2000.\n",
      "Step: 1639 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1639.\n",
      "Refiner model loss: [1.74804462 0.92691893 0.82112568].\n",
      "Discriminator model loss real: [0.38431626].\n",
      "Discriminator model loss refined: [0.29122878].\n",
      "Step: 1640 of 2000.\n",
      "Step: 1641 of 2000.\n",
      "Step: 1642 of 2000.\n",
      "Step: 1643 of 2000.\n",
      "Step: 1644 of 2000.\n",
      "Step: 1645 of 2000.\n",
      "Step: 1646 of 2000.\n",
      "Step: 1647 of 2000.\n",
      "Step: 1648 of 2000.\n",
      "Step: 1649 of 2000.\n",
      "Step: 1650 of 2000.\n",
      "Step: 1651 of 2000.\n",
      "Step: 1652 of 2000.\n",
      "Step: 1653 of 2000.\n",
      "Step: 1654 of 2000.\n",
      "Step: 1655 of 2000.\n",
      "Step: 1656 of 2000.\n",
      "Step: 1657 of 2000.\n",
      "Step: 1658 of 2000.\n",
      "Step: 1659 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1659.\n",
      "Refiner model loss: [1.74679321 0.92455871 0.8222345 ].\n",
      "Discriminator model loss real: [0.38475589].\n",
      "Discriminator model loss refined: [0.29086592].\n",
      "Step: 1660 of 2000.\n",
      "Step: 1661 of 2000.\n",
      "Step: 1662 of 2000.\n",
      "Step: 1663 of 2000.\n",
      "Step: 1664 of 2000.\n",
      "Step: 1665 of 2000.\n",
      "Step: 1666 of 2000.\n",
      "Step: 1667 of 2000.\n",
      "Step: 1668 of 2000.\n",
      "Step: 1669 of 2000.\n",
      "Step: 1670 of 2000.\n",
      "Step: 1671 of 2000.\n",
      "Step: 1672 of 2000.\n",
      "Step: 1673 of 2000.\n",
      "Step: 1674 of 2000.\n",
      "Step: 1675 of 2000.\n",
      "Step: 1676 of 2000.\n",
      "Step: 1677 of 2000.\n",
      "Step: 1678 of 2000.\n",
      "Step: 1679 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1679.\n",
      "Refiner model loss: [1.74722404 0.92381263 0.82341143].\n",
      "Discriminator model loss real: [0.38444032].\n",
      "Discriminator model loss refined: [0.2905025].\n",
      "Step: 1680 of 2000.\n",
      "Step: 1681 of 2000.\n",
      "Step: 1682 of 2000.\n",
      "Step: 1683 of 2000.\n",
      "Step: 1684 of 2000.\n",
      "Step: 1685 of 2000.\n",
      "Step: 1686 of 2000.\n",
      "Step: 1687 of 2000.\n",
      "Step: 1688 of 2000.\n",
      "Step: 1689 of 2000.\n",
      "Step: 1690 of 2000.\n",
      "Step: 1691 of 2000.\n",
      "Step: 1692 of 2000.\n",
      "Step: 1693 of 2000.\n",
      "Step: 1694 of 2000.\n",
      "Step: 1695 of 2000.\n",
      "Step: 1696 of 2000.\n",
      "Step: 1697 of 2000.\n",
      "Step: 1698 of 2000.\n",
      "Step: 1699 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1699.\n",
      "Refiner model loss: [1.74691988 0.92269446 0.82422542].\n",
      "Discriminator model loss real: [0.38488564].\n",
      "Discriminator model loss refined: [0.2898876].\n",
      "Step: 1700 of 2000.\n",
      "Step: 1701 of 2000.\n",
      "Step: 1702 of 2000.\n",
      "Step: 1703 of 2000.\n",
      "Step: 1704 of 2000.\n",
      "Step: 1705 of 2000.\n",
      "Step: 1706 of 2000.\n",
      "Step: 1707 of 2000.\n",
      "Step: 1708 of 2000.\n",
      "Step: 1709 of 2000.\n",
      "Step: 1710 of 2000.\n",
      "Step: 1711 of 2000.\n",
      "Step: 1712 of 2000.\n",
      "Step: 1713 of 2000.\n",
      "Step: 1714 of 2000.\n",
      "Step: 1715 of 2000.\n",
      "Step: 1716 of 2000.\n",
      "Step: 1717 of 2000.\n",
      "Step: 1718 of 2000.\n",
      "Step: 1719 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1719.\n",
      "Refiner model loss: [1.74558049 0.92032918 0.8252513 ].\n",
      "Discriminator model loss real: [0.38524858].\n",
      "Discriminator model loss refined: [0.28961634].\n",
      "Step: 1720 of 2000.\n",
      "Step: 1721 of 2000.\n",
      "Step: 1722 of 2000.\n",
      "Step: 1723 of 2000.\n",
      "Step: 1724 of 2000.\n",
      "Step: 1725 of 2000.\n",
      "Step: 1726 of 2000.\n",
      "Step: 1727 of 2000.\n",
      "Step: 1728 of 2000.\n",
      "Step: 1729 of 2000.\n",
      "Step: 1730 of 2000.\n",
      "Step: 1731 of 2000.\n",
      "Step: 1732 of 2000.\n",
      "Step: 1733 of 2000.\n",
      "Step: 1734 of 2000.\n",
      "Step: 1735 of 2000.\n",
      "Step: 1736 of 2000.\n",
      "Step: 1737 of 2000.\n",
      "Step: 1738 of 2000.\n",
      "Step: 1739 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1739.\n",
      "Refiner model loss: [1.74289745 0.9165706  0.82632685].\n",
      "Discriminator model loss real: [0.38553454].\n",
      "Discriminator model loss refined: [0.28912428].\n",
      "Step: 1740 of 2000.\n",
      "Step: 1741 of 2000.\n",
      "Step: 1742 of 2000.\n",
      "Step: 1743 of 2000.\n",
      "Step: 1744 of 2000.\n",
      "Step: 1745 of 2000.\n",
      "Step: 1746 of 2000.\n",
      "Step: 1747 of 2000.\n",
      "Step: 1748 of 2000.\n",
      "Step: 1749 of 2000.\n",
      "Step: 1750 of 2000.\n",
      "Step: 1751 of 2000.\n",
      "Step: 1752 of 2000.\n",
      "Step: 1753 of 2000.\n",
      "Step: 1754 of 2000.\n",
      "Step: 1755 of 2000.\n",
      "Step: 1756 of 2000.\n",
      "Step: 1757 of 2000.\n",
      "Step: 1758 of 2000.\n",
      "Step: 1759 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1759.\n",
      "Refiner model loss: [1.74401527 0.91657549 0.82743978].\n",
      "Discriminator model loss real: [0.38612965].\n",
      "Discriminator model loss refined: [0.28897509].\n",
      "Step: 1760 of 2000.\n",
      "Step: 1761 of 2000.\n",
      "Step: 1762 of 2000.\n",
      "Step: 1763 of 2000.\n",
      "Step: 1764 of 2000.\n",
      "Step: 1765 of 2000.\n",
      "Step: 1766 of 2000.\n",
      "Step: 1767 of 2000.\n",
      "Step: 1768 of 2000.\n",
      "Step: 1769 of 2000.\n",
      "Step: 1770 of 2000.\n",
      "Step: 1771 of 2000.\n",
      "Step: 1772 of 2000.\n",
      "Step: 1773 of 2000.\n",
      "Step: 1774 of 2000.\n",
      "Step: 1775 of 2000.\n",
      "Step: 1776 of 2000.\n",
      "Step: 1777 of 2000.\n",
      "Step: 1778 of 2000.\n",
      "Step: 1779 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1779.\n",
      "Refiner model loss: [1.74267419 0.91411663 0.82855756].\n",
      "Discriminator model loss real: [0.38581023].\n",
      "Discriminator model loss refined: [0.28856628].\n",
      "Step: 1780 of 2000.\n",
      "Step: 1781 of 2000.\n",
      "Step: 1782 of 2000.\n",
      "Step: 1783 of 2000.\n",
      "Step: 1784 of 2000.\n",
      "Step: 1785 of 2000.\n",
      "Step: 1786 of 2000.\n",
      "Step: 1787 of 2000.\n",
      "Step: 1788 of 2000.\n",
      "Step: 1789 of 2000.\n",
      "Step: 1790 of 2000.\n",
      "Step: 1791 of 2000.\n",
      "Step: 1792 of 2000.\n",
      "Step: 1793 of 2000.\n",
      "Step: 1794 of 2000.\n",
      "Step: 1795 of 2000.\n",
      "Step: 1796 of 2000.\n",
      "Step: 1797 of 2000.\n",
      "Step: 1798 of 2000.\n",
      "Step: 1799 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1799.\n",
      "Refiner model loss: [1.74006951 0.91033425 0.82973526].\n",
      "Discriminator model loss real: [0.3864001].\n",
      "Discriminator model loss refined: [0.28816034].\n",
      "Step: 1800 of 2000.\n",
      "Step: 1801 of 2000.\n",
      "Step: 1802 of 2000.\n",
      "Step: 1803 of 2000.\n",
      "Step: 1804 of 2000.\n",
      "Step: 1805 of 2000.\n",
      "Step: 1806 of 2000.\n",
      "Step: 1807 of 2000.\n",
      "Step: 1808 of 2000.\n",
      "Step: 1809 of 2000.\n",
      "Step: 1810 of 2000.\n",
      "Step: 1811 of 2000.\n",
      "Step: 1812 of 2000.\n",
      "Step: 1813 of 2000.\n",
      "Step: 1814 of 2000.\n",
      "Step: 1815 of 2000.\n",
      "Step: 1816 of 2000.\n",
      "Step: 1817 of 2000.\n",
      "Step: 1818 of 2000.\n",
      "Step: 1819 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1819.\n",
      "Refiner model loss: [1.74378637 0.91334449 0.83044188].\n",
      "Discriminator model loss real: [0.38676938].\n",
      "Discriminator model loss refined: [0.28761225].\n",
      "Step: 1820 of 2000.\n",
      "Step: 1821 of 2000.\n",
      "Step: 1822 of 2000.\n",
      "Step: 1823 of 2000.\n",
      "Step: 1824 of 2000.\n",
      "Step: 1825 of 2000.\n",
      "Step: 1826 of 2000.\n",
      "Step: 1827 of 2000.\n",
      "Step: 1828 of 2000.\n",
      "Step: 1829 of 2000.\n",
      "Step: 1830 of 2000.\n",
      "Step: 1831 of 2000.\n",
      "Step: 1832 of 2000.\n",
      "Step: 1833 of 2000.\n",
      "Step: 1834 of 2000.\n",
      "Step: 1835 of 2000.\n",
      "Step: 1836 of 2000.\n",
      "Step: 1837 of 2000.\n",
      "Step: 1838 of 2000.\n",
      "Step: 1839 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1839.\n",
      "Refiner model loss: [1.74288475 0.91127428 0.83161048].\n",
      "Discriminator model loss real: [0.38670256].\n",
      "Discriminator model loss refined: [0.28750583].\n",
      "Step: 1840 of 2000.\n",
      "Step: 1841 of 2000.\n",
      "Step: 1842 of 2000.\n",
      "Step: 1843 of 2000.\n",
      "Step: 1844 of 2000.\n",
      "Step: 1845 of 2000.\n",
      "Step: 1846 of 2000.\n",
      "Step: 1847 of 2000.\n",
      "Step: 1848 of 2000.\n",
      "Step: 1849 of 2000.\n",
      "Step: 1850 of 2000.\n",
      "Step: 1851 of 2000.\n",
      "Step: 1852 of 2000.\n",
      "Step: 1853 of 2000.\n",
      "Step: 1854 of 2000.\n",
      "Step: 1855 of 2000.\n",
      "Step: 1856 of 2000.\n",
      "Step: 1857 of 2000.\n",
      "Step: 1858 of 2000.\n",
      "Step: 1859 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1859.\n",
      "Refiner model loss: [1.74131247 0.90887506 0.8324374 ].\n",
      "Discriminator model loss real: [0.38734111].\n",
      "Discriminator model loss refined: [0.28726265].\n",
      "Step: 1860 of 2000.\n",
      "Step: 1861 of 2000.\n",
      "Step: 1862 of 2000.\n",
      "Step: 1863 of 2000.\n",
      "Step: 1864 of 2000.\n",
      "Step: 1865 of 2000.\n",
      "Step: 1866 of 2000.\n",
      "Step: 1867 of 2000.\n",
      "Step: 1868 of 2000.\n",
      "Step: 1869 of 2000.\n",
      "Step: 1870 of 2000.\n",
      "Step: 1871 of 2000.\n",
      "Step: 1872 of 2000.\n",
      "Step: 1873 of 2000.\n",
      "Step: 1874 of 2000.\n",
      "Step: 1875 of 2000.\n",
      "Step: 1876 of 2000.\n",
      "Step: 1877 of 2000.\n",
      "Step: 1878 of 2000.\n",
      "Step: 1879 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1879.\n",
      "Refiner model loss: [1.73974134 0.9060725  0.83366884].\n",
      "Discriminator model loss real: [0.38731086].\n",
      "Discriminator model loss refined: [0.28700427].\n",
      "Step: 1880 of 2000.\n",
      "Step: 1881 of 2000.\n",
      "Step: 1882 of 2000.\n",
      "Step: 1883 of 2000.\n",
      "Step: 1884 of 2000.\n",
      "Step: 1885 of 2000.\n",
      "Step: 1886 of 2000.\n",
      "Step: 1887 of 2000.\n",
      "Step: 1888 of 2000.\n",
      "Step: 1889 of 2000.\n",
      "Step: 1890 of 2000.\n",
      "Step: 1891 of 2000.\n",
      "Step: 1892 of 2000.\n",
      "Step: 1893 of 2000.\n",
      "Step: 1894 of 2000.\n",
      "Step: 1895 of 2000.\n",
      "Step: 1896 of 2000.\n",
      "Step: 1897 of 2000.\n",
      "Step: 1898 of 2000.\n",
      "Step: 1899 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1899.\n",
      "Refiner model loss: [1.7386824  0.90415511 0.83452728].\n",
      "Discriminator model loss real: [0.38780648].\n",
      "Discriminator model loss refined: [0.28670278].\n",
      "Step: 1900 of 2000.\n",
      "Step: 1901 of 2000.\n",
      "Step: 1902 of 2000.\n",
      "Step: 1903 of 2000.\n",
      "Step: 1904 of 2000.\n",
      "Step: 1905 of 2000.\n",
      "Step: 1906 of 2000.\n",
      "Step: 1907 of 2000.\n",
      "Step: 1908 of 2000.\n",
      "Step: 1909 of 2000.\n",
      "Step: 1910 of 2000.\n",
      "Step: 1911 of 2000.\n",
      "Step: 1912 of 2000.\n",
      "Step: 1913 of 2000.\n",
      "Step: 1914 of 2000.\n",
      "Step: 1915 of 2000.\n",
      "Step: 1916 of 2000.\n",
      "Step: 1917 of 2000.\n",
      "Step: 1918 of 2000.\n",
      "Step: 1919 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1919.\n",
      "Refiner model loss: [1.74129047 0.90608112 0.83520935].\n",
      "Discriminator model loss real: [0.38764242].\n",
      "Discriminator model loss refined: [0.28634253].\n",
      "Step: 1920 of 2000.\n",
      "Step: 1921 of 2000.\n",
      "Step: 1922 of 2000.\n",
      "Step: 1923 of 2000.\n",
      "Step: 1924 of 2000.\n",
      "Step: 1925 of 2000.\n",
      "Step: 1926 of 2000.\n",
      "Step: 1927 of 2000.\n",
      "Step: 1928 of 2000.\n",
      "Step: 1929 of 2000.\n",
      "Step: 1930 of 2000.\n",
      "Step: 1931 of 2000.\n",
      "Step: 1932 of 2000.\n",
      "Step: 1933 of 2000.\n",
      "Step: 1934 of 2000.\n",
      "Step: 1935 of 2000.\n",
      "Step: 1936 of 2000.\n",
      "Step: 1937 of 2000.\n",
      "Step: 1938 of 2000.\n",
      "Step: 1939 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1939.\n",
      "Refiner model loss: [1.73989954 0.90386126 0.83603828].\n",
      "Discriminator model loss real: [0.38838287].\n",
      "Discriminator model loss refined: [0.28557335].\n",
      "Step: 1940 of 2000.\n",
      "Step: 1941 of 2000.\n",
      "Step: 1942 of 2000.\n",
      "Step: 1943 of 2000.\n",
      "Step: 1944 of 2000.\n",
      "Step: 1945 of 2000.\n",
      "Step: 1946 of 2000.\n",
      "Step: 1947 of 2000.\n",
      "Step: 1948 of 2000.\n",
      "Step: 1949 of 2000.\n",
      "Step: 1950 of 2000.\n",
      "Step: 1951 of 2000.\n",
      "Step: 1952 of 2000.\n",
      "Step: 1953 of 2000.\n",
      "Step: 1954 of 2000.\n",
      "Step: 1955 of 2000.\n",
      "Step: 1956 of 2000.\n",
      "Step: 1957 of 2000.\n",
      "Step: 1958 of 2000.\n",
      "Step: 1959 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1959.\n",
      "Refiner model loss: [1.74175482 0.9047627  0.83699211].\n",
      "Discriminator model loss real: [0.38829645].\n",
      "Discriminator model loss refined: [0.28558274].\n",
      "Step: 1960 of 2000.\n",
      "Step: 1961 of 2000.\n",
      "Step: 1962 of 2000.\n",
      "Step: 1963 of 2000.\n",
      "Step: 1964 of 2000.\n",
      "Step: 1965 of 2000.\n",
      "Step: 1966 of 2000.\n",
      "Step: 1967 of 2000.\n",
      "Step: 1968 of 2000.\n",
      "Step: 1969 of 2000.\n",
      "Step: 1970 of 2000.\n",
      "Step: 1971 of 2000.\n",
      "Step: 1972 of 2000.\n",
      "Step: 1973 of 2000.\n",
      "Step: 1974 of 2000.\n",
      "Step: 1975 of 2000.\n",
      "Step: 1976 of 2000.\n",
      "Step: 1977 of 2000.\n",
      "Step: 1978 of 2000.\n",
      "Step: 1979 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1979.\n",
      "Refiner model loss: [1.74029761 0.90231527 0.83798234].\n",
      "Discriminator model loss real: [0.38797677].\n",
      "Discriminator model loss refined: [0.28510409].\n",
      "Step: 1980 of 2000.\n",
      "Step: 1981 of 2000.\n",
      "Step: 1982 of 2000.\n",
      "Step: 1983 of 2000.\n",
      "Step: 1984 of 2000.\n",
      "Step: 1985 of 2000.\n",
      "Step: 1986 of 2000.\n",
      "Step: 1987 of 2000.\n",
      "Step: 1988 of 2000.\n",
      "Step: 1989 of 2000.\n",
      "Step: 1990 of 2000.\n",
      "Step: 1991 of 2000.\n",
      "Step: 1992 of 2000.\n",
      "Step: 1993 of 2000.\n",
      "Step: 1994 of 2000.\n",
      "Step: 1995 of 2000.\n",
      "Step: 1996 of 2000.\n",
      "Step: 1997 of 2000.\n",
      "Step: 1998 of 2000.\n",
      "Step: 1999 of 2000.\n",
      "Saving batch of refined images at adversarial step: 1999.\n",
      "Refiner model loss: [1.73979996 0.90119019 0.83860976].\n",
      "Discriminator model loss real: [0.38853027].\n",
      "Discriminator model loss refined: [0.28471855].\n"
     ]
    }
   ],
   "source": [
    "# TODO: what is an appropriate size for the image history buffer?\n",
    "image_history_buffer = ImageHistoryBuffer((0, img_height, img_width, img_channels), batch_size * 100, batch_size)\n",
    "\n",
    "combined_loss = np.zeros(shape=len(combined_model.metrics_names))\n",
    "disc_loss_real = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "disc_loss_refined = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "# see Algorithm 1 in https://arxiv.org/pdf/1612.07828v1.pdf\n",
    "for i in range(nb_steps):\n",
    "    print('Step: {} of {}.'.format(i, nb_steps))\n",
    "\n",
    "    # train the refiner\n",
    "    for _ in range(k_g * 2):\n",
    "        # sample a mini-batch of synthetic images\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "\n",
    "        # update θ by taking an SGD step on mini-batch loss LR(θ)\n",
    "        combined_loss = np.add(combined_model.train_on_batch(synthetic_image_batch,\n",
    "                                                             [synthetic_image_batch, y_real]), combined_loss)\n",
    "\n",
    "    for _ in range(k_d):\n",
    "        # sample a mini-batch of synthetic and real images\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        real_image_batch = get_image_batch(real_generator)\n",
    "\n",
    "        # refine the synthetic images w/ the current refiner\n",
    "        refined_image_batch = refiner_model.predict_on_batch(synthetic_image_batch)\n",
    "\n",
    "        # use a history of refined images\n",
    "        half_batch_from_image_history = image_history_buffer.get_from_image_history_buffer()\n",
    "        image_history_buffer.add_to_image_history_buffer(refined_image_batch)\n",
    "\n",
    "        if len(half_batch_from_image_history):\n",
    "            refined_image_batch[:batch_size // 2] = half_batch_from_image_history\n",
    "\n",
    "        # update φ by taking an SGD step on mini-batch loss LD(φ)\n",
    "        disc_loss_real = np.add(discriminator_model.train_on_batch(real_image_batch, y_real), disc_loss_real)\n",
    "        disc_loss_refined = np.add(discriminator_model.train_on_batch(refined_image_batch, y_refined),\n",
    "                                   disc_loss_refined)\n",
    "\n",
    "    if not (i+1) % log_interval:\n",
    "        # plot batch of refined images w/ current refiner\n",
    "        figure_name = 'refined_image_batch_step_{}.png'.format(i)\n",
    "        print('Saving batch of refined images at adversarial step: {}.'.format(i))\n",
    "\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        plot_batch(\n",
    "            np.concatenate((synthetic_image_batch, refiner_model.predict_on_batch(synthetic_image_batch))),\n",
    "            os.path.join(cache_dir, figure_name),\n",
    "            label_batch=['Synthetic'] * batch_size + ['Refined'] * batch_size)\n",
    "\n",
    "        # log loss summary\n",
    "        print('Refiner model loss: {}.'.format(combined_loss / (log_interval * k_g * 2)))\n",
    "        Refiner_model_loss.append(combined_loss / (log_interval * k_g * 2))\n",
    "        print('Discriminator model loss real: {}.'.format(disc_loss_real / (log_interval * k_d * 2)))\n",
    "        Discriminator_model_loss_real.append(disc_loss_real / (log_interval * k_d * 2))\n",
    "        print('Discriminator model loss refined: {}.'.format(disc_loss_refined / (log_interval * k_d * 2)))\n",
    "        Discriminator_model_loss_refined.append(disc_loss_refined / (log_interval * k_d * 2))\n",
    "\n",
    "        combined_loss = np.zeros(shape=len(combined_model.metrics_names))\n",
    "        disc_loss_real = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "        disc_loss_refined = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "    # save model checkpoints\n",
    "    if not (i+1) % save_interval:\n",
    "        model_checkpoint_base_name = os.path.join(cache_dir, '{}_model_step_{}.h5')\n",
    "        refiner_model.save(model_checkpoint_base_name.format('refiner', i))\n",
    "        discriminator_model.save(model_checkpoint_base_name.format('discriminator', i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_image_batch(generator, length):\n",
    "    \"\"\"keras generators may generate an incomplete batch for the last batch\"\"\"\n",
    "    img_batch = generator.next()\n",
    "    \n",
    "    if len(img_batch) != length:\n",
    "        img_batch = generator.next()\n",
    "\n",
    "    assert len(img_batch) == length\n",
    "\n",
    "    return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_flow_params = {'batch_size': 50}\n",
    "my_synthetic_generator = datagen.flow(\n",
    "    x = syn_image_stack,\n",
    "    **my_flow_params\n",
    "    )\n",
    "my_synthetic_image_batch = my_get_image_batch(my_synthetic_generator, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = refiner_model.predict_on_batch(my_synthetic_image_batch)\n",
    "di = discriminator_model.predict_on_batch(my_synthetic_image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Images found: 10\n",
      "(10, 240, 320, 1)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('cal.h5','r') as t_file:\n",
    "    assert 'image' in t_file, \"Images are missing\"\n",
    "    print('Real Images found:',len(t_file['image']))\n",
    "    for _, (ikey, ival) in zip(range(1), t_file.items()):\n",
    "        img_height = ival.shape[1]\n",
    "        img_width = ival.shape[2]\n",
    "        img_channels = 1\n",
    "    tmp2 = np.stack([np.expand_dims(a,-1) for a in t_file.values()],0)\n",
    "    cal_image_stack = tmp2[0]\n",
    "    print(cal_image_stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cal_flow_params = {'batch_size': 10}\n",
    "my_cal_generator = datagen.flow(\n",
    "    x = cal_image_stack,\n",
    "    **my_cal_flow_params\n",
    "    )\n",
    "my_cal_image_batch = my_get_image_batch(my_cal_generator, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = refiner_model.predict_on_batch(my_cal_image_batch)\n",
    "di = discriminator_model.predict_on_batch(my_cal_image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25edf5b5e08>,\n",
       " <matplotlib.lines.Line2D at 0x25edf5b5f88>,\n",
       " <matplotlib.lines.Line2D at 0x25edf5c0188>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Refiner_model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25edf615f48>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Discriminator_model_loss_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25edf683f48>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Discriminator_model_loss_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Refiner_model_loss\n",
    "Discriminator_model_loss_real\n",
    "Discriminator_model_loss_refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(10):\n",
    "    number = i\n",
    "    resized_img = cv2.resize(re[i],(320,240))\n",
    "    name = str(number) + '.png'\n",
    "    plt.imsave(name, resized_img, cmap = cm.gray)\n",
    "    \n",
    "\n",
    "for i in range(10):\n",
    "    number = i\n",
    "    resized_img = cv2.resize(my_cal_image_batch[i],(320,240))\n",
    "    name = str(number) + '_syn.png'\n",
    "    plt.imsave(name, resized_img, cmap = cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = refiner_model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "refiner_model.save_weights(\"model.h5\")\n",
    "print(\"Saved model into h5 file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model =  keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss=self_regularization_loss, optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = h5py.File('new_test.h5','r')\n",
    "\n",
    "tmp = np.stack([np.expand_dims(a,-1) for a in archive.values()],0)\n",
    "my_syn_image_stack = tmp[0]\n",
    "# syn_image_stack = np.array([np.expand_dims(a,-1) for a in archive.values()])\n",
    "print(my_syn_image_stack.shape)\n",
    "\n",
    "archive.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_synthetic_generator = datagen.flow(\n",
    "    x = my_syn_image_stack,\n",
    "    **flow_params\n",
    ")\n",
    "\n",
    "my_synthetic_image_batch = get_image_batch(my_synthetic_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelot\n",
    "ath = 'C:\\\\Users\\\\localadmin\\\\Documents\\\\Yifan\\\\UnityEyes_Windows\\\\imgs\\\\'\n",
    "\n",
    "array = []\n",
    "for i in range(1,50):\n",
    "    filename = filepath+str(i)+'.jpg'\n",
    "    img = cv2.imread(filename,cv2.IMREAD_GRAYSCALE)\n",
    "    r_img = image_resize(img, width = 55, height = 35)\n",
    "    resized_image = cv2.resize(r_img, (55, 35)) \n",
    "    array.append(resized_image)\n",
    "\n",
    "archive = h5py.File('new_test.h5', 'w')\n",
    "archive['image'] = array\n",
    "\n",
    "archive.close()"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
